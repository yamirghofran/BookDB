{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bd52311",
   "metadata": {},
   "source": [
    "# Uploading Embeddings to Qdrand\n",
    "In this notebook, we will upload the following embeddings to qdrant\n",
    "- SBERT Book Metadata embeddings\n",
    "- GMF User embeddings\n",
    "- GMF Book embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98459851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcc01c3",
   "metadata": {},
   "source": [
    "## 1. Remap GMF Embeddings to original User and Item ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_map = pd.read_csv('data/user_id_map_reduced.csv')\n",
    "item_id_map = pd.read_csv('data/item_id_map_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b2164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35886881",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_user_embeddings_df = dd.read_parquet(\"embeddings/gmf_user_embeddings.parquet\")\n",
    "gmf_user_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the embedding columns (assuming they are '0' to '31')\n",
    "embedding_cols = [str(i) for i in range(32)]\n",
    "\n",
    "# Define a function to combine embedding columns into a list\n",
    "def combine_embeddings(row):\n",
    "    return row[embedding_cols].tolist()\n",
    "\n",
    "# Apply the function row-wise to create the 'embedding' column\n",
    "# meta specifies the output column name and data type for Dask\n",
    "gmf_user_embeddings_df['embedding'] = gmf_user_embeddings_df.apply(\n",
    "    combine_embeddings,\n",
    "    axis=1,\n",
    "    meta=('embedding', 'object')\n",
    ")\n",
    "\n",
    "# Select the user_id and the new embedding column, dropping the old ones\n",
    "gmf_user_embeddings_final = gmf_user_embeddings_df[['user_id', 'embedding']]\n",
    "\n",
    "# Display the head of the transformed DataFrame\n",
    "gmf_user_embeddings_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a438de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge user embeddings with user ID map\n",
    "gmf_user_embeddings_final = gmf_user_embeddings_final.merge(\n",
    "    user_id_map,\n",
    "    left_on='user_id',\n",
    "    right_on='new_userId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Select and rename columns\n",
    "gmf_user_embeddings_final = gmf_user_embeddings_final[['original_userId', 'embedding']]\n",
    "gmf_user_embeddings_final = gmf_user_embeddings_final.rename(columns={'original_userId': 'user_id'})\n",
    "\n",
    "# Display the head of the remapped user embeddings DataFrame\n",
    "gmf_user_embeddings_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmf_book_embeddings_df = dd.read_parquet(\"embeddings/gmf_book_embeddings.parquet\")\n",
    "gmf_book_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af2506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the embedding columns (assuming they are '0' to '31')\n",
    "embedding_cols = [str(i) for i in range(32)]\n",
    "\n",
    "# Define a function to combine embedding columns into a list\n",
    "def combine_embeddings(row):\n",
    "    return row[embedding_cols].tolist()\n",
    "\n",
    "# Apply the function row-wise to create the 'embedding' column\n",
    "# meta specifies the output column name and data type for Dask\n",
    "gmf_book_embeddings_df['embedding'] = gmf_book_embeddings_df.apply(\n",
    "    combine_embeddings,\n",
    "    axis=1,\n",
    "    meta=('embedding', 'object')\n",
    ")\n",
    "\n",
    "# Select the item_id and the new embedding column, dropping the old ones\n",
    "gmf_book_embeddings_final = gmf_book_embeddings_df[['item_id', 'embedding']]\n",
    "\n",
    "# Display the head of the transformed DataFrame\n",
    "gmf_book_embeddings_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868119b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge book embeddings with item ID map\n",
    "gmf_book_embeddings_final = gmf_book_embeddings_final.merge(\n",
    "    item_id_map,\n",
    "    left_on='item_id',\n",
    "    right_on='new_itemId',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Select and rename columns\n",
    "gmf_book_embeddings_final = gmf_book_embeddings_final[['original_itemId', 'embedding']]\n",
    "gmf_book_embeddings_final = gmf_book_embeddings_final.rename(columns={'original_itemId': 'item_id'})\n",
    "\n",
    "# Display the head of the remapped book embeddings DataFrame\n",
    "gmf_book_embeddings_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a0e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_embeddings_df = dd.read_parquet(\"embeddings/sbert_embeddings.parquet\")\n",
    "sbert_embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed935b",
   "metadata": {},
   "source": [
    "## Setting Up Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c76190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"sbert_embeddings\",\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"gmf_user_embeddings\",\n",
    "    vectors_config=VectorParams(size=32, distance=Distance.DOT),\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"gmf_book_embeddings\",\n",
    "    vectors_config=VectorParams(size=32, distance=Distance.DOT),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdde179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"gmf_book\", gmf_book_embeddings_final.columns)\n",
    "print(\"gmf_user\", gmf_user_embeddings_final.columns)\n",
    "print(\"sbert_embeddings\", sbert_embeddings_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ce7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "import uuid\n",
    "\n",
    "batch_size = 500 # Define batch size for uploads\n",
    "\n",
    "# --- Upload GMF User Embeddings ---\n",
    "print(\"Uploading GMF User Embeddings...\")\n",
    "user_points_to_upload = []\n",
    "# Compute the Dask DataFrame\n",
    "computed_user_df = gmf_user_embeddings_final.compute()\n",
    "for index, row in computed_user_df.iterrows():\n",
    "    # User IDs are strings (hashes) based on previous error\n",
    "    user_id_val = str(row['user_id']) # Ensure it's treated as a string\n",
    "    user_points_to_upload.append(PointStruct(\n",
    "        id=user_id_val, # Use string user_id as the point ID\n",
    "        vector=row['embedding'],\n",
    "        payload={\"user_id\": user_id_val} # Store string user_id in payload\n",
    "    ))\n",
    "\n",
    "# Upsert GMF User embeddings in batches\n",
    "print(f\"Upserting {len(user_points_to_upload)} GMF user points in batches of {batch_size}...\")\n",
    "for i in range(0, len(user_points_to_upload), batch_size):\n",
    "    batch = user_points_to_upload[i:i + batch_size]\n",
    "    client.upsert(collection_name=\"gmf_user_embeddings\", points=batch, wait=True)\n",
    "print(f\"Uploaded {len(user_points_to_upload)} GMF user points.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Upload GMF Book Embeddings ---\n",
    "print(\"Uploading GMF Book Embeddings...\")\n",
    "book_points_to_upload = []\n",
    "# Compute the Dask DataFrame\n",
    "computed_book_df = gmf_book_embeddings_final.compute()\n",
    "correct_book_id_column_gmf = 'item_id' # Based on previous output\n",
    "\n",
    "for index, row in computed_book_df.iterrows():\n",
    "    try:\n",
    "        # Assume item_id is integer, handle potential errors\n",
    "        point_id = int(row[correct_book_id_column_gmf])\n",
    "        payload_id = point_id\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not convert ID '{row[correct_book_id_column_gmf]}' to int for GMF book embedding. Using as string.\")\n",
    "        point_id = str(row[correct_book_id_column_gmf])\n",
    "        payload_id = point_id\n",
    "\n",
    "    book_points_to_upload.append(PointStruct(\n",
    "        id=point_id, # Use original item_id as the point ID (int or string)\n",
    "        vector=row['embedding'],\n",
    "        payload={correct_book_id_column_gmf: payload_id} # Store item_id in payload\n",
    "    ))\n",
    "\n",
    "# Upsert GMF Book embeddings in batches\n",
    "print(f\"Upserting {len(book_points_to_upload)} GMF book points in batches of {batch_size}...\")\n",
    "for i in range(0, len(book_points_to_upload), batch_size):\n",
    "    batch = book_points_to_upload[i:i + batch_size]\n",
    "    client.upsert(collection_name=\"gmf_book_embeddings\", points=batch, wait=True)\n",
    "print(f\"Uploaded {len(book_points_to_upload)} GMF book points.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Upload SBERT Book Embeddings ---\n",
    "print(\"Uploading SBERT Book Embeddings...\")\n",
    "sbert_points_to_upload = []\n",
    "# Compute the Dask DataFrame\n",
    "computed_sbert_df = sbert_embeddings_df.compute()\n",
    "correct_book_id_column_sbert = 'book_id' # Based on previous output\n",
    "\n",
    "for index, row in computed_sbert_df.iterrows():\n",
    "    try:\n",
    "        # Assume book_id is integer, handle potential errors\n",
    "        point_id = int(row[correct_book_id_column_sbert])\n",
    "        payload_id = point_id\n",
    "    except ValueError:\n",
    "        print(f\"Warning: Could not convert ID '{row[correct_book_id_column_sbert]}' to int for SBERT embedding. Using as string.\")\n",
    "        point_id = str(row[correct_book_id_column_sbert])\n",
    "        payload_id = point_id\n",
    "\n",
    "    sbert_points_to_upload.append(PointStruct(\n",
    "        id=point_id, # Use the determined point ID (int or string)\n",
    "        vector=row['embedding'], # Assuming 'embedding' column is correct\n",
    "        payload={\n",
    "            correct_book_id_column_sbert: payload_id,\n",
    "            \"text\": row.get(\"text\", \"\") # Include 'text' in payload, handle if missing\n",
    "        }\n",
    "    ))\n",
    "\n",
    "# Upsert SBERT embeddings in batches\n",
    "print(f\"Upserting {len(sbert_points_to_upload)} SBERT book points in batches of {batch_size}...\")\n",
    "for i in range(0, len(sbert_points_to_upload), batch_size):\n",
    "    batch = sbert_points_to_upload[i:i + batch_size]\n",
    "    client.upsert(collection_name=\"sbert_embeddings\", points=batch, wait=True)\n",
    "print(f\"Uploaded {len(sbert_points_to_upload)} SBERT book points.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(\"All uploads complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
