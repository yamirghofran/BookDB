{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Encoder Fine-Tuning & Evaluation Notebook\n",
    "\n",
    "This notebook outlines the end-to-end process for fine-tuning a Sentence-Transformers `CrossEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anze/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = 'data/processed_training_pairs_parts_0_to_12.parquet'\n",
    "output_model_dir = 'reranker_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 222360, Val pairs: 27792, Test pairs: 27804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Load all pairs and drop book_id\n",
    "df = pd.read_parquet(data_path)\n",
    "df = df.drop('book_id', axis=1)\n",
    "\n",
    "# 2) Get unique users\n",
    "users = df['user_id'].unique()\n",
    "\n",
    "# 3) First split: 80% train, 20% temp (val+test)\n",
    "train_users, temp_users = train_test_split(\n",
    "    users, \n",
    "    test_size=0.2, \n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# 4) Second split: half of temp → val (10%), half → test (10%)\n",
    "val_users, test_users = train_test_split(\n",
    "    temp_users, \n",
    "    test_size=0.5, \n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# 5) Build DataFrames\n",
    "train_df = df[df['user_id'].isin(train_users)].reset_index(drop=True)\n",
    "val_df   = df[df['user_id'].isin(val_users)].reset_index(drop=True)\n",
    "test_df  = df[df['user_id'].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train pairs: {len(train_df)}, Val pairs: {len(val_df)}, Test pairs: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_ctx</th>\n",
       "      <th>book_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Down and Out in Paris and London | Genr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Virgin Suicides | Genres: coming-of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Seducing Cinderella (Fighting for Love,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Highlander Untamed (MacLeods of Skye Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: A Woman in Berlin: Eight Weeks in the C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  \\\n",
       "0  001af7947e217e17694c5a9c097afffb   \n",
       "1  001af7947e217e17694c5a9c097afffb   \n",
       "2  001af7947e217e17694c5a9c097afffb   \n",
       "3  001af7947e217e17694c5a9c097afffb   \n",
       "4  001af7947e217e17694c5a9c097afffb   \n",
       "\n",
       "                                            user_ctx  \\\n",
       "0  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "1  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "2  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "3  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "4  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "\n",
       "                                           book_text  label  \n",
       "0  Title: Down and Out in Paris and London | Genr...      1  \n",
       "1  Title: The Virgin Suicides | Genres: coming-of...      0  \n",
       "2  Title: Seducing Cinderella (Fighting for Love,...      0  \n",
       "3  Title: Highlander Untamed (MacLeods of Skye Tr...      0  \n",
       "4  Title: A Woman in Berlin: Eight Weeks in the C...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare InputExamples & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to InputExample\n",
    "train_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in train_df.itertuples()\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in val_df.itertuples()\n",
    "]\n",
    "\n",
    "test_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in test_df.itertuples()\n",
    "]\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader   = DataLoader(val_examples, shuffle=False, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_examples, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Pre Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Baseline Model: Pre-trained CrossEncoder ---\n",
      "Using test_df with shape: (27804, 4) for pre-trained baseline evaluation.\n",
      "Successfully loaded pre-trained model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Making predictions with the pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 869/869 [02:33<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pre-trained CrossEncoder Baseline Results ---\n",
      "Global ROC AUC: 0.5988\n",
      "Global Average Precision (across all items): 0.4133\n",
      "Mean Average Precision (MAP): 0.5921\n",
      "Mean Per-User NDCG@3: 0.4806\n",
      "Mean Per-User NDCG@5: 0.5731\n",
      "Mean Per-User NDCG@10: 0.6940\n",
      "--- Baseline Model: Pre-trained CrossEncoder Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Baseline Model: Pre-trained CrossEncoder (No Fine-Tuning) ---\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Starting Baseline Model: Pre-trained CrossEncoder ---\")\n",
    "\n",
    "# Initialize metric variables for baseline\n",
    "baseline_global_roc_auc = None\n",
    "baseline_global_ap = None\n",
    "baseline_map = None\n",
    "baseline_ndcg_3 = None\n",
    "baseline_ndcg_5 = None\n",
    "baseline_ndcg_10 = None\n",
    "\n",
    "# --- Configuration ---\n",
    "PRETRAINED_MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2' \n",
    "MAX_LENGTH_PRETRAINED = 256\n",
    "\n",
    "if 'test_df' not in locals() or not isinstance(test_df, pd.DataFrame) or test_df.empty:\n",
    "    print(\"Error: `test_df` is not defined, not a DataFrame, or is empty.\")\n",
    "    print(\"Please ensure `test_df` is created and populated from your data splitting cells before running this baseline.\")\n",
    "else:\n",
    "    print(f\"Using test_df with shape: {test_df.shape} for pre-trained baseline evaluation.\")\n",
    "    baseline_model_instance = None # Renamed to avoid conflict if 'baseline_model' is used elsewhere\n",
    "    try:\n",
    "        baseline_model_instance = CrossEncoder(\n",
    "            PRETRAINED_MODEL_NAME,\n",
    "            max_length=MAX_LENGTH_PRETRAINED\n",
    "        )\n",
    "        print(f\"Successfully loaded pre-trained model: {PRETRAINED_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pre-trained model {PRETRAINED_MODEL_NAME}: {e}\")\n",
    "\n",
    "    if baseline_model_instance:\n",
    "        if 'user_ctx' not in test_df.columns or 'book_text' not in test_df.columns:\n",
    "            print(\"Error: `test_df` is missing 'user_ctx' or 'book_text' columns.\")\n",
    "        else:\n",
    "            test_pairs_for_baseline = [[row.user_ctx, row.book_text] for row in test_df.itertuples()]\n",
    "            \n",
    "            if not test_pairs_for_baseline:\n",
    "                print(\"Error: `test_pairs_for_baseline` list is empty. Cannot make predictions.\")\n",
    "            else:\n",
    "                print(\"Making predictions with the pre-trained model...\")\n",
    "                baseline_scores_pred = [] \n",
    "                try:\n",
    "                    baseline_scores_pred = baseline_model_instance.predict(test_pairs_for_baseline, show_progress_bar=True)\n",
    "                    \n",
    "                    if 'label' not in test_df.columns:\n",
    "                        print(\"Error: `test_df` is missing 'label' column for evaluation.\")\n",
    "                    else:\n",
    "                        test_true_labels = test_df['label'].values\n",
    "\n",
    "                        if len(baseline_scores_pred) == len(test_true_labels):\n",
    "                            print(f\"\\n--- Pre-trained CrossEncoder Baseline Results ---\")\n",
    "                            \n",
    "                            baseline_global_roc_auc = roc_auc_score(test_true_labels, baseline_scores_pred)\n",
    "                            baseline_global_ap = average_precision_score(test_true_labels, baseline_scores_pred)\n",
    "                            print(f\"Global ROC AUC: {baseline_global_roc_auc:.4f}\")\n",
    "                            print(f\"Global Average Precision (across all items): {baseline_global_ap:.4f}\")\n",
    "\n",
    "                            user_ndcg_scores_at_3_bl = [] # Suffix _bl for baseline\n",
    "                            user_ndcg_scores_at_5_bl = []\n",
    "                            user_ndcg_scores_at_10_bl = []\n",
    "                            user_ap_scores_bl = []\n",
    "\n",
    "                            if 'user_id' in test_df.columns:\n",
    "                                evaluation_df_baseline = pd.DataFrame({\n",
    "                                    'user_id': test_df['user_id'], \n",
    "                                    'label': test_true_labels,      \n",
    "                                    'score': baseline_scores_pred \n",
    "                                })\n",
    "\n",
    "                                for user_id_val in evaluation_df_baseline['user_id'].unique():\n",
    "                                    user_data = evaluation_df_baseline[evaluation_df_baseline['user_id'] == user_id_val]\n",
    "                                    \n",
    "                                    if not user_data.empty and user_data['label'].sum() > 0:\n",
    "                                        y_true_user_list = user_data['label'].values.tolist()\n",
    "                                        y_score_user_list = user_data['score'].values.tolist()\n",
    "\n",
    "                                        user_ap = average_precision_score(y_true_user_list, y_score_user_list)\n",
    "                                        user_ap_scores_bl.append(user_ap)\n",
    "                                        \n",
    "                                        y_true_user_ndcg = [y_true_user_list]\n",
    "                                        y_score_user_ndcg = [y_score_user_list]\n",
    "                                        \n",
    "                                        k3 = min(3, len(user_data))\n",
    "                                        if k3 > 0:\n",
    "                                            user_ndcg_3_val = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k3) # temp var\n",
    "                                            user_ndcg_scores_at_3_bl.append(user_ndcg_3_val)\n",
    "                                        \n",
    "                                        k5 = min(5, len(user_data))\n",
    "                                        if k5 > 0:\n",
    "                                            user_ndcg_5_val = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k5) # temp var\n",
    "                                            user_ndcg_scores_at_5_bl.append(user_ndcg_5_val)\n",
    "\n",
    "                                        k10 = min(10, len(user_data))\n",
    "                                        if k10 > 0:\n",
    "                                             user_ndcg_10_val = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k10) # temp var\n",
    "                                             user_ndcg_scores_at_10_bl.append(user_ndcg_10_val)\n",
    "                                \n",
    "                                if user_ap_scores_bl:\n",
    "                                    baseline_map = np.mean(user_ap_scores_bl)\n",
    "                                    print(f\"Mean Average Precision (MAP): {baseline_map:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"MAP could not be calculated (no valid user AP scores).\")\n",
    "\n",
    "                                if user_ndcg_scores_at_3_bl:\n",
    "                                    baseline_ndcg_3 = np.mean(user_ndcg_scores_at_3_bl)\n",
    "                                    print(f\"Mean Per-User NDCG@3: {baseline_ndcg_3:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"Mean Per-User NDCG@3 could not be calculated.\")\n",
    "                                \n",
    "                                if user_ndcg_scores_at_5_bl:\n",
    "                                    baseline_ndcg_5 = np.mean(user_ndcg_scores_at_5_bl)\n",
    "                                    print(f\"Mean Per-User NDCG@5: {baseline_ndcg_5:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"Mean Per-User NDCG@5 could not be calculated.\")\n",
    "\n",
    "                                if user_ndcg_scores_at_10_bl: \n",
    "                                    baseline_ndcg_10 = np.mean(user_ndcg_scores_at_10_bl)\n",
    "                                    print(f\"Mean Per-User NDCG@10: {baseline_ndcg_10:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"Mean Per-User NDCG@10 could not be calculated.\")\n",
    "                            else:\n",
    "                                print(\"Error: 'user_id' column not found in test_df. Cannot calculate per-user metrics.\")\n",
    "                        else:\n",
    "                            print(f\"Error: Mismatch in length between predicted scores ({len(baseline_scores_pred)}) and true labels ({len(test_true_labels)}). Evaluation skipped.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during prediction or evaluation with pre-trained model: {e}\")\n",
    "\n",
    "print(\"--- Baseline Model: Pre-trained CrossEncoder Finished ---\")\n",
    "\n",
    "# Verification print for baseline metrics\n",
    "print(\"\\n--- Saved Metrics for Baseline Model (for later comparison) ---\")\n",
    "print(f\"baseline_global_roc_auc: {baseline_global_roc_auc}\")\n",
    "print(f\"baseline_global_ap: {baseline_global_ap}\")\n",
    "print(f\"baseline_map: {baseline_map}\")\n",
    "print(f\"baseline_ndcg_3: {baseline_ndcg_3}\")\n",
    "print(f\"baseline_ndcg_5: {baseline_ndcg_5}\")\n",
    "print(f\"baseline_ndcg_10: {baseline_ndcg_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Validation Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import CrossEncoderEvaluator\n",
    "\n",
    "evaluator = CrossEncoderEvaluator.from_input_examples(\n",
    "    val_examples,     \n",
    "    name='val',\n",
    "    batch_size=16,\n",
    "    main_score_function=lambda y_true, y_pred: ndcg_score([y_true], [y_pred], k=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate & Fine-Tune CrossEncoder\n",
    "\n",
    "cross-encoder/ms-marco-MiniLM-L-6-v2 is a 6-layer MiniLM distilled into a cross-encoder architecture and pretrained on the MS MARCO passage ranking task. It takes a paired input (e.g. user context + book text) and produces a single relevance score via full token-level attention.\n",
    "\n",
    "Justification\n",
    "\t•\tRanking-Tuned Pretraining\n",
    "Its MS MARCO heritage means it already knows how to judge fine-grained relevance patterns—crucial for matching nuanced book descriptions to user tastes.\n",
    "\t•\tSpeed-Quality Sweet Spot\n",
    "At ~60 MB and with inference under 15 ms per candidate, it delivers ~90–95 % of full BERT-base accuracy, keeping end-to-end latency low.\n",
    "\t•\tEfficient Fine-Tuning\n",
    "Requires only 2–3 epochs over ~150 K (user,book) pairs to adapt deeply to book-domain language, making rapid iteration feasible.\n",
    "\t•\tCompact & Deployable\n",
    "Its small footprint simplifies packaging, loading, and scaling in production environments with moderate memory and compute budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps per epoch: 13898\n",
      "Calculated warmup steps (10% of one epoch): 1389\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# 1. Calculate the number of training steps per epoch\n",
    "num_train_steps_per_epoch = len(train_dataloader)\n",
    "print(f\"Number of training steps per epoch: {num_train_steps_per_epoch}\")\n",
    "\n",
    "# 2. Calculate warmup steps\n",
    "warmup_ratio = 0.10  # 10% for warmup\n",
    "actual_warmup_steps = int(num_train_steps_per_epoch * warmup_ratio)\n",
    "print(f\"Calculated warmup steps (10% of one epoch): {actual_warmup_steps}\")\n",
    "\n",
    "# Ensure warmup_steps is not zero if num_train_steps_per_epoch is very small\n",
    "if actual_warmup_steps == 0 and num_train_steps_per_epoch > 0:\n",
    "    actual_warmup_steps = 1 # Ensure at least 1 step if training is happening\n",
    "    print(f\"Adjusted warmup steps to 1 as calculated value was 0.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder(\n",
    "    'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    num_labels=1,\n",
    "    max_length=384,  \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=epochs, # e.g., 3\n",
    "    learning_rate=learning_rate, # e.g., 2e-5\n",
    "    scheduler='WarmupLinear', \n",
    "    warmup_steps=actual_warmup_steps, \n",
    "    evaluation_steps=num_train_steps_per_epoch,  \n",
    "    output_path=output_model_dir,\n",
    "    save_best_model=True,\n",
    "    use_amp=True,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # Make sure this is imported\n",
    "\n",
    "# --- Plotting Validation Performance for CrossEncoder ---\n",
    "print(\"\\n--- Plotting CrossEncoder Validation Performance ---\")\n",
    "\n",
    "# Define where your model.fit() saved its output\n",
    "# This should be the same as the 'output_path' you gave to model.fit()\n",
    "output_model_dir_from_training = output_model_dir # Or directly use 'reranker_model' if that's the variable\n",
    "\n",
    "# The evaluator was named 'val' in your setup\n",
    "evaluator_name = 'val' \n",
    "eval_filename = f\"{evaluator_name}_results.csv\"\n",
    "\n",
    "# Path to the evaluation results CSV file\n",
    "# It's typically directly in the output_path for CrossEncoder, or sometimes in an 'eval' subfolder.\n",
    "# Check your output_model_dir after training to confirm the exact path.\n",
    "# Common paths:\n",
    "# 1. output_model_dir / evaluator_name_results.csv\n",
    "# 2. output_model_dir / eval / evaluator_name_results.csv\n",
    "\n",
    "# Let's try the first common path:\n",
    "eval_filepath = os.path.join(output_model_dir_from_training, eval_filename)\n",
    "\n",
    "# If not found, try the 'eval' subdirectory (less common for CrossEncoder's default save)\n",
    "if not os.path.exists(eval_filepath):\n",
    "    eval_filepath_alt = os.path.join(output_model_dir_from_training, \"eval\", eval_filename)\n",
    "    if os.path.exists(eval_filepath_alt):\n",
    "        eval_filepath = eval_filepath_alt\n",
    "    else:\n",
    "        print(f\"Could not find evaluation file at {eval_filepath} or {eval_filepath_alt}\")\n",
    "        # Set eval_filepath to a non-existent path to trigger FileNotFoundError below if needed\n",
    "        eval_filepath = os.path.join(output_model_dir_from_training, \"FILE_DOES_NOT_EXIST.csv\")\n",
    "\n",
    "\n",
    "# The main score column in the CSV will match the metric from your main_score_function\n",
    "# Your main_score_function was ndcg_score with k=3.\n",
    "# The column name is often the name of the evaluator + \"_\" + the metric name (or just the metric name).\n",
    "# For NDCG from ndcg_score, it might be saved as 'ndcg' or 'score', or 'val_ndcg'.\n",
    "# You'll need to INSPECT the CSV file after one training run to get the exact column name.\n",
    "# Let's assume for now it's called 'score' (as it's the output of main_score_function) or 'ndcg'.\n",
    "\n",
    "# Common potential column names for the score from your main_score_function\n",
    "# The CrossEncoderEvaluator often names the column after the evaluator's name plus the metric.\n",
    "# If main_score_function just returns a score, it might be 'score' or based on the lambda.\n",
    "# Let's try a few common ones, or you can inspect the CSV header.\n",
    "potential_score_column_names = [\n",
    "    f'{evaluator_name}_ndcg', # e.g., val_ndcg if ndcg_score returns a dict key\n",
    "    'score',                 # Generic name if the main_score_function directly returns the score\n",
    "    'ndcg',                  # If the metric itself is named 'ndcg'\n",
    "    # Add other possibilities based on inspecting your CSV file\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "    eval_results_df = pd.read_csv(eval_filepath)\n",
    "    print(f\"Successfully loaded evaluation results from: {eval_filepath}\")\n",
    "    print(\"Columns in evaluation CSV:\", eval_results_df.columns.tolist())\n",
    "\n",
    "    # Determine the actual score column name\n",
    "    score_column = None\n",
    "    for col_name in potential_score_column_names:\n",
    "        if col_name in eval_results_df.columns:\n",
    "            score_column = col_name\n",
    "            break\n",
    "    \n",
    "    if 'epoch' in eval_results_df.columns and 'steps' in eval_results_df.columns and score_column:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot score vs steps\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(eval_results_df['steps'], eval_results_df[score_column], marker='o', linestyle='-')\n",
    "        plt.title(f'Validation {score_column.replace(\"_\", \" \").title()} vs. Training Steps')\n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel(score_column.replace(\"_\", \" \").title())\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot score vs epoch\n",
    "        plt.subplot(1, 2, 2)\n",
    "        epoch_end_evals = eval_results_df.loc[eval_results_df.groupby('epoch')['steps'].idxmax()]\n",
    "        plt.plot(epoch_end_evals['epoch'], epoch_end_evals[score_column], marker='o', linestyle='-')\n",
    "        plt.title(f'Validation {score_column.replace(\"_\", \" \").title()} vs. Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(score_column.replace(\"_\", \" \").title())\n",
    "        plt.xticks(epoch_end_evals['epoch'].unique()) \n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        # Ensure output_model_dir_from_training is defined correctly\n",
    "        plot_save_path = os.path.join(output_model_dir_from_training, f'{evaluator_name}_performance_plot.png')\n",
    "        plt.savefig(plot_save_path)\n",
    "        print(f\"Validation plot saved to: {plot_save_path}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        missing_cols = []\n",
    "        if 'epoch' not in eval_results_df.columns: missing_cols.append('epoch')\n",
    "        if 'steps' not in eval_results_df.columns: missing_cols.append('steps')\n",
    "        if not score_column: missing_cols.append(f\"one of {potential_score_column_names}\")\n",
    "        print(f\"Required columns {missing_cols} not found in {eval_filepath}. Cannot plot.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Evaluation results file not found. Checked paths ending with {eval_filename}\")\n",
    "    print(\"Plotting skipped. Ensure `output_path` was set in `model.fit()` and training completed at least one evaluation step.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during plotting: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Evaluating Fine-Tuned CrossEncoder Model on Test Set ---\")\n",
    "\n",
    "# Initialize metric variables to None or a default (e.g., 0.0) in case evaluation fails\n",
    "finetuned_global_roc_auc = None\n",
    "finetuned_global_ap = None\n",
    "finetuned_map = None\n",
    "finetuned_ndcg_3 = None\n",
    "finetuned_ndcg_5 = None\n",
    "finetuned_ndcg_10 = None\n",
    "\n",
    "if 'test_df' not in locals() or not isinstance(test_df, pd.DataFrame) or test_df.empty:\n",
    "    print(\"Error: `test_df` is not defined, not a DataFrame, or is empty.\")\n",
    "    print(\"Please ensure `test_df` is available for final model evaluation.\")\n",
    "else:\n",
    "    if 'user_ctx' not in test_df.columns or 'book_text' not in test_df.columns or 'label' not in test_df.columns:\n",
    "        print(\"Error: `test_df` is missing one or more required columns: 'user_ctx', 'book_text', 'label'.\")\n",
    "    else:\n",
    "        # 1. Prepare test pairs for prediction\n",
    "        test_pairs_for_model = [[row.user_ctx, row.book_text] for row in test_df.itertuples()]\n",
    "        \n",
    "        if not test_pairs_for_model:\n",
    "            print(\"Error: `test_pairs_for_model` list is empty. Cannot make predictions.\")\n",
    "        else:\n",
    "            print(\"Making predictions with the fine-tuned model...\")\n",
    "            # 2. Get Predictions from the fine-tuned model\n",
    "            # Make sure your 'model' variable refers to the trained model instance\n",
    "            try:\n",
    "                model_scores = model.predict(test_pairs_for_model, show_progress_bar=True)\n",
    "                true_labels = test_df['label'].values\n",
    "\n",
    "                if len(model_scores) == len(true_labels):\n",
    "                    print(f\"\\n--- Fine-Tuned Model Test Set Results ---\")\n",
    "\n",
    "                    # 3. Compute and Store Global Metrics\n",
    "                    finetuned_global_roc_auc = roc_auc_score(true_labels, model_scores)\n",
    "                    finetuned_global_ap = average_precision_score(true_labels, model_scores)\n",
    "                    print(f\"Global ROC AUC: {finetuned_global_roc_auc:.4f}\")\n",
    "                    print(f\"Global Average Precision (across all items): {finetuned_global_ap:.4f}\")\n",
    "\n",
    "                    # 4. Compute and Store Per-User Metrics (MAP, NDCG@3, NDCG@5, NDCG@10)\n",
    "                    user_ap_scores_ft = []\n",
    "                    user_ndcg_scores_at_3_ft = []\n",
    "                    user_ndcg_scores_at_5_ft = []\n",
    "                    user_ndcg_scores_at_10_ft = []\n",
    "\n",
    "                    if 'user_id' in test_df.columns:\n",
    "                        evaluation_df_ft = pd.DataFrame({\n",
    "                            'user_id': test_df['user_id'],\n",
    "                            'label': true_labels,\n",
    "                            'score': model_scores\n",
    "                        })\n",
    "\n",
    "                        for user_id_val in evaluation_df_ft['user_id'].unique():\n",
    "                            user_data = evaluation_df_ft[evaluation_df_ft['user_id'] == user_id_val]\n",
    "\n",
    "                            if not user_data.empty and user_data['label'].sum() > 0:\n",
    "                                y_true_user_list = user_data['label'].values.tolist()\n",
    "                                y_score_user_list = user_data['score'].values.tolist()\n",
    "\n",
    "                                user_ap = average_precision_score(y_true_user_list, y_score_user_list)\n",
    "                                user_ap_scores_ft.append(user_ap)\n",
    "\n",
    "                                y_true_user_ndcg = [y_true_user_list]\n",
    "                                y_score_user_ndcg = [y_score_user_list]\n",
    "\n",
    "                                k3 = min(3, len(user_data))\n",
    "                                if k3 > 0:\n",
    "                                    user_ndcg_3 = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k3)\n",
    "                                    user_ndcg_scores_at_3_ft.append(user_ndcg_3)\n",
    "                                \n",
    "                                k5 = min(5, len(user_data))\n",
    "                                if k5 > 0:\n",
    "                                    user_ndcg_5 = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k5)\n",
    "                                    user_ndcg_scores_at_5_ft.append(user_ndcg_5)\n",
    "\n",
    "                                k10 = min(10, len(user_data))\n",
    "                                if k10 > 0:\n",
    "                                    user_ndcg_10 = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k10)\n",
    "                                    user_ndcg_scores_at_10_ft.append(user_ndcg_10)\n",
    "                        \n",
    "                        if user_ap_scores_ft:\n",
    "                            finetuned_map = np.mean(user_ap_scores_ft)\n",
    "                            print(f\"Mean Average Precision (MAP): {finetuned_map:.4f}\")\n",
    "                        else:\n",
    "                            print(\"MAP could not be calculated.\")\n",
    "\n",
    "                        if user_ndcg_scores_at_3_ft:\n",
    "                            finetuned_ndcg_3 = np.mean(user_ndcg_scores_at_3_ft)\n",
    "                            print(f\"Mean Per-User NDCG@3: {finetuned_ndcg_3:.4f}\")\n",
    "                        else:\n",
    "                            print(\"Mean Per-User NDCG@3 could not be calculated.\")\n",
    "                        \n",
    "                        if user_ndcg_scores_at_5_ft:\n",
    "                            finetuned_ndcg_5 = np.mean(user_ndcg_scores_at_5_ft)\n",
    "                            print(f\"Mean Per-User NDCG@5: {finetuned_ndcg_5:.4f}\")\n",
    "                        else:\n",
    "                            print(\"Mean Per-User NDCG@5 could not be calculated.\")\n",
    "\n",
    "                        if user_ndcg_scores_at_10_ft:\n",
    "                            finetuned_ndcg_10 = np.mean(user_ndcg_scores_at_10_ft)\n",
    "                            print(f\"Mean Per-User NDCG@10: {finetuned_ndcg_10:.4f}\")\n",
    "                        else:\n",
    "                            print(\"Mean Per-User NDCG@10 could not be calculated.\")\n",
    "                    else:\n",
    "                        print(\"Error: 'user_id' column not found in test_df. Cannot calculate per-user metrics.\")\n",
    "                else:\n",
    "                    print(f\"Error: Mismatch in length between model_scores ({len(model_scores)}) and true_labels ({len(true_labels)}). Evaluation skipped.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction or evaluation with the fine-tuned model: {e}\")\n",
    "\n",
    "print(\"--- Fine-Tuned Model Evaluation Finished ---\")\n",
    "\n",
    "# You can now verify the variables:\n",
    "print(\"\\n--- Saved Metrics for Fine-Tuned Model (for later comparison) ---\")\n",
    "print(f\"finetuned_global_roc_auc: {finetuned_global_roc_auc}\")\n",
    "print(f\"finetuned_global_ap: {finetuned_global_ap}\")\n",
    "print(f\"finetuned_map: {finetuned_map}\")\n",
    "print(f\"finetuned_ndcg_3: {finetuned_ndcg_3}\")\n",
    "print(f\"finetuned_ndcg_5: {finetuned_ndcg_5}\")\n",
    "print(f\"finetuned_ndcg_10: {finetuned_ndcg_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Metrics from Baseline Model ---\n",
    "baseline_metrics = {\n",
    "    'Global ROC AUC': baseline_global_roc_auc,\n",
    "    'Global AP': baseline_global_ap,\n",
    "    'MAP': baseline_map,\n",
    "    'NDCG@3': baseline_ndcg_3,\n",
    "    'NDCG@5': baseline_ndcg_5,\n",
    "    'NDCG@10': baseline_ndcg_10\n",
    "}\n",
    "\n",
    "# --- Metrics from Fine-Tuned Model ---\n",
    "finetuned_metrics = {\n",
    "    'Global ROC AUC': finetuned_global_roc_auc,  \n",
    "    'Global AP': finetuned_global_ap,       \n",
    "    'MAP': finetuned_map,             \n",
    "    'NDCG@3': finetuned_ndcg_3,          \n",
    "    'NDCG@5': finetuned_ndcg_5,           \n",
    "    'NDCG@10': finetuned_ndcg_10         \n",
    "}\n",
    "\n",
    "# 1. Create a Comparison Table\n",
    "metrics_data = {\n",
    "    'Metric': list(baseline_metrics.keys()),\n",
    "    'Baseline': list(baseline_metrics.values()),\n",
    "    'Fine-Tuned': list(finetuned_metrics.values())\n",
    "}\n",
    "comparison_df = pd.DataFrame(metrics_data)\n",
    "comparison_df['Improvement'] = comparison_df['Fine-Tuned'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement (%)'] = (comparison_df['Improvement'] / comparison_df['Baseline']) * 100\n",
    "comparison_df['Improvement (%)'] = comparison_df['Improvement (%)'].round(2)\n",
    "\n",
    "print(\"--- Model Performance Comparison ---\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# 2. Generate Bar Charts for Visual Comparison\n",
    "metric_names = list(baseline_metrics.keys())\n",
    "baseline_values = list(baseline_metrics.values())\n",
    "finetuned_values = list(finetuned_metrics.values())\n",
    "\n",
    "num_metrics = len(metric_names)\n",
    "bar_width = 0.35\n",
    "index = np.arange(num_metrics)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "rects1 = ax.bar(index - bar_width/2, baseline_values, bar_width, label='Baseline', color='skyblue')\n",
    "rects2 = ax.bar(index + bar_width/2, finetuned_values, bar_width, label='Fine-Tuned', color='coral')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Baseline vs. Fine-Tuned Model Performance by Metric')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=8)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "key_per_user_metrics = ['MAP', 'NDCG@3', 'NDCG@5', 'NDCG@10']\n",
    "key_baseline_values = [baseline_metrics[m] for m in key_per_user_metrics]\n",
    "key_finetuned_values = [finetuned_metrics[m] for m in key_per_user_metrics]\n",
    "\n",
    "num_key_metrics = len(key_per_user_metrics)\n",
    "index_key = np.arange(num_key_metrics)\n",
    "\n",
    "fig_key, ax_key = plt.subplots(figsize=(10, 6))\n",
    "rects1_key = ax_key.bar(index_key - bar_width/2, key_baseline_values, bar_width, label='Baseline', color='skyblue')\n",
    "rects2_key = ax_key.bar(index_key + bar_width/2, key_finetuned_values, bar_width, label='Fine-Tuned', color='coral')\n",
    "\n",
    "ax_key.set_ylabel('Scores')\n",
    "ax_key.set_title('Key Per-User Metrics: Baseline vs. Fine-Tuned')\n",
    "ax_key.set_xticks(index_key)\n",
    "ax_key.set_xticklabels(key_per_user_metrics)\n",
    "ax_key.legend()\n",
    "\n",
    "autolabel(rects1_key)\n",
    "autolabel(rects2_key)\n",
    "\n",
    "fig_key.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is already saved during fit\n",
    "# To load:\n",
    "from sentence_transformers import CrossEncoder\n",
    "loaded_model = CrossEncoder(output_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a single user and its candidates\n",
    "user_ctx = \"Favorite books: ...\"  # fetched or precomputed\n",
    "candidate_texts = [\"Title: ... Description: ...\", ...]\n",
    "pairs = [[user_ctx, txt] for txt in candidate_texts]\n",
    "scores = loaded_model.predict(pairs)\n",
    "\n",
    "# Rerank\n",
    "candidates = ['book1', 'book2', ...]\n",
    "ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "print(ranked[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
