{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Encoder Fine-Tuning & Evaluation Notebook\n",
    "\n",
    "This notebook outlines the end-to-end process for fine-tuning a Sentence-Transformers `CrossEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anze/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "def download_from_r2(object_name, local_path, bucket_name=\"bookdbio\"):\n",
    "    # ensure parent dir exists\n",
    "    parent_dir = os.path.dirname(local_path)\n",
    "    if parent_dir and not os.path.isdir(parent_dir):\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "        endpoint_url = f\"https://a9a190ee80813000e18bacf626b1281b.r2.cloudflarestorage.com/\",\n",
    "        aws_access_key_id = '85fec6dd1268801ac8c1c59175ba0b76',\n",
    "        aws_secret_access_key = '798b753bab748f2c7f5e0f46fd6506b7f0b206e362b1e00055d060a72b88d55d',\n",
    "        config = Config(signature_version='s3v4')\n",
    "   )\n",
    "\n",
    "    try:\n",
    "        s3.download_file(bucket_name, object_name, local_path)\n",
    "        print(f\"Successfully downloaded {object_name} to {local_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed for {object_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/training_pairs.parquet.zip to data/training_pairs.parquet.zip\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/training_pairs.parquet.zip\", \"data/training_pairs.parquet.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Define the base path to your Parquet directory\n",
    "base_path = 'data/training_pairs.parquet/' # Make sure this ends with a slash\n",
    "\n",
    "# List the specific parts you want to load\n",
    "parts_to_load = [\n",
    "    base_path + 'part.0.parquet',\n",
    "    base_path + 'part.1.parquet',\n",
    "    base_path + 'part.2.parquet',\n",
    "    base_path + 'part.3.parquet',\n",
    "    base_path + 'part.4.parquet',\n",
    "    base_path + 'part.5.parquet',\n",
    "    base_path + 'part.6.parquet',\n",
    "    base_path + 'part.7.parquet',\n",
    "    base_path + 'part.8.parquet',\n",
    "    base_path + 'part.9.parquet',\n",
    "    base_path + 'part.10.parquet',\n",
    "    base_path + 'part.11.parquet',\n",
    "    base_path + 'part.12.parquet',\n",
    "]\n",
    "\n",
    "# Load the specified parts\n",
    "df_dd = dd.read_parquet(parts_to_load)\n",
    "\n",
    "# You can then compute it to a Pandas DataFrame if needed for further processing\n",
    "# or use Dask operations directly.\n",
    "training_pairs_df = df_dd.compute()\n",
    "\n",
    "print(f\"Dask DataFrame loaded with {df_dd.npartitions} partitions.\")\n",
    "# print(df_pd.head())\n",
    "# print(f\"Shape of loaded data: {df_pd.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_ctx</th>\n",
       "      <th>book_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>57854</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Tao Te Ching | Genres: history, literat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>15808287</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Mrs. Lincoln's Dressmaker | Genres: bio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>3692</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Heart of the Matter | Genres: conte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>603515</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Hound of Rowan (The Tapestry, #1) |...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>34</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Fellowship of the Ring (The Lord of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>73965</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Drinking: A Love Story | Genres: biogra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>1215919</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Highlander Untamed (MacLeods of Skye Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>218038</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: All About Love (Cynster, #6) | Genres: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>7332</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Silmarillion | Genres: adventure, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>455930</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Echo Burning (Jack Reacher, #5) | Genre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>6145711</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Walking Dead, Vol. 10: What We Beco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>23361172</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Paperweight | Genres: contemporary, dra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>5470</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: 1984 | Genres: dystopian, fantasy, horr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>343529</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: On a Wild Night (Cynster, #8) | Genres:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>16057629</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Seducing Cinderella (Fighting for Love,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>73716</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Como agua para chocolate | Genres: cont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>9646</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Homage to Catalonia | Genres: biography...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>3077927</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: ماجدولين | Genres: drama, literature, r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>916856</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Miss Pettigrew Lives for a Day | Genres...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>14142</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Art of Loving | Genres: education, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             user_id   book_id  \\\n",
       "0   001af7947e217e17694c5a9c097afffb     57854   \n",
       "1   001af7947e217e17694c5a9c097afffb  15808287   \n",
       "2   001af7947e217e17694c5a9c097afffb      3692   \n",
       "3   001af7947e217e17694c5a9c097afffb    603515   \n",
       "4   001af7947e217e17694c5a9c097afffb        34   \n",
       "5   001af7947e217e17694c5a9c097afffb     73965   \n",
       "6   001af7947e217e17694c5a9c097afffb   1215919   \n",
       "7   001af7947e217e17694c5a9c097afffb    218038   \n",
       "8   001af7947e217e17694c5a9c097afffb      7332   \n",
       "9   001af7947e217e17694c5a9c097afffb    455930   \n",
       "10  001af7947e217e17694c5a9c097afffb   6145711   \n",
       "11  001af7947e217e17694c5a9c097afffb  23361172   \n",
       "12  001af7947e217e17694c5a9c097afffb      5470   \n",
       "13  001af7947e217e17694c5a9c097afffb    343529   \n",
       "14  001af7947e217e17694c5a9c097afffb  16057629   \n",
       "15  001af7947e217e17694c5a9c097afffb     73716   \n",
       "16  001af7947e217e17694c5a9c097afffb      9646   \n",
       "17  001af7947e217e17694c5a9c097afffb   3077927   \n",
       "18  001af7947e217e17694c5a9c097afffb    916856   \n",
       "19  001af7947e217e17694c5a9c097afffb     14142   \n",
       "\n",
       "                                             user_ctx  \\\n",
       "0   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "1   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "2   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "3   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "4   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "5   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "6   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "7   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "8   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "9   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "10  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "11  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "12  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "13  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "14  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "15  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "16  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "17  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "18  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "19  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "\n",
       "                                            book_text  label  \n",
       "0   Title: Tao Te Ching | Genres: history, literat...      1  \n",
       "1   Title: Mrs. Lincoln's Dressmaker | Genres: bio...      0  \n",
       "2   Title: The Heart of the Matter | Genres: conte...      0  \n",
       "3   Title: The Hound of Rowan (The Tapestry, #1) |...      0  \n",
       "4   Title: The Fellowship of the Ring (The Lord of...      1  \n",
       "5   Title: Drinking: A Love Story | Genres: biogra...      0  \n",
       "6   Title: Highlander Untamed (MacLeods of Skye Tr...      0  \n",
       "7   Title: All About Love (Cynster, #6) | Genres: ...      0  \n",
       "8   Title: The Silmarillion | Genres: adventure, a...      1  \n",
       "9   Title: Echo Burning (Jack Reacher, #5) | Genre...      0  \n",
       "10  Title: The Walking Dead, Vol. 10: What We Beco...      0  \n",
       "11  Title: Paperweight | Genres: contemporary, dra...      0  \n",
       "12  Title: 1984 | Genres: dystopian, fantasy, horr...      1  \n",
       "13  Title: On a Wild Night (Cynster, #8) | Genres:...      0  \n",
       "14  Title: Seducing Cinderella (Fighting for Love,...      0  \n",
       "15  Title: Como agua para chocolate | Genres: cont...      0  \n",
       "16  Title: Homage to Catalonia | Genres: biography...      1  \n",
       "17  Title: ماجدولين | Genres: drama, literature, r...      0  \n",
       "18  Title: Miss Pettigrew Lives for a Day | Genres...      0  \n",
       "19  Title: The Art of Loving | Genres: education, ...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded data: (2862473, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of loaded data: {training_pairs_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (936681, 5)\n",
      "\n",
      "Columns: ['user_id', 'book_id', 'user_ctx', 'book_text', 'label']\n",
      "\n",
      "Sample of the data:\n",
      "                            user_id   book_id  \\\n",
      "0  001af7947e217e17694c5a9c097afffb     57854   \n",
      "1  001af7947e217e17694c5a9c097afffb  15808287   \n",
      "2  001af7947e217e17694c5a9c097afffb      3692   \n",
      "3  001af7947e217e17694c5a9c097afffb    603515   \n",
      "4  001af7947e217e17694c5a9c097afffb        34   \n",
      "\n",
      "                                            user_ctx  \\\n",
      "0  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "1  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "2  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "3  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "4  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "\n",
      "                                           book_text  label  \n",
      "0  Title: Tao Te Ching | Genres: history, literat...      1  \n",
      "1  Title: Mrs. Lincoln's Dressmaker | Genres: bio...      0  \n",
      "2  Title: The Heart of the Matter | Genres: conte...      0  \n",
      "3  Title: The Hound of Rowan (The Tapestry, #1) |...      0  \n",
      "4  Title: The Fellowship of the Ring (The Lord of...      1  \n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    698112\n",
      "1    238569\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example user context:\n",
      "Favorite books: Tao Te Ching by Lao Tzu and Gia-Fu Feng and Jane English and Chungliang Al Huang and Rowena Pattee Kryder and Toinette Lippe, The Fellowship of the Ring (The Lord of the Rings, #1) by J.R.R. Tolkien and The Silmarillion by J.R.R. Tolkien and Christopher Tolkien and Ted Nasmith. Favorite genres: literature and history.\n",
      "\n",
      "Example book text:\n",
      "Title: Tao Te Ching | Genres: history, literature, mythology, nonfiction, philosophy, poetry, self-help, spiritual | Description: The Tao Te Ching, the esoteric but infinitely practical book written most probably in the sixth century B.C. by Lao Tsu, has been translated more frequently than any work except the Bible. This translation of the Chinese classic, which was first published twenty-five years ago, has sold more copies than any of the others. It offers the essence of each word and makes Lao Tsu's teaching immediate and alive.\n",
      "The philosophy of Lao Tsu is simple: Accept what is in front of you without wanting the situation to be other than it is. Study the natural order of things and work with it rather than against it, for to try to change what isonly sets up resistance. Nature provides everything without requiring payment or thanks, and also provides for all without discrimination--therefore let us present the same face to everyone and treat all men as equals, however they may behave. If we watch carefully, we will see that work proceeds more quickly and easily if we stop \"trying,\" if we stop putting in so much extra effort, if we stop looking for results. In the clarity of a still and open mind, truth will be reflected. We will come to appreciate the original meaning of the word \"understand,\" which means \"to stand under.\" We serve whatever or whoever stands before us, without any thought for ourselves. Te--which may be translated as \"virtue\" or \"strength\"--lies always in Tao,or\" natural law. In other words: Simply be. | Authors: Lao Tzu, Gia-Fu Feng, Jane English, Chungliang Al Huang, Rowena Pattee Kryder, Toinette Lippe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the training pairs\n",
    "training_pairs = dd.read_parquet('data/training_pairs.parquet/part.0.parquet')\n",
    "\n",
    "# Convert to pandas for easier inspection\n",
    "training_pairs_pd = training_pairs.compute()\n",
    "\n",
    "# Display basic information\n",
    "print(\"Shape of the dataset:\", training_pairs_pd.shape)\n",
    "print(\"\\nColumns:\", training_pairs_pd.columns.tolist())\n",
    "print(\"\\nSample of the data:\")\n",
    "print(training_pairs_pd.head())\n",
    "\n",
    "# Check the distribution of labels\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(training_pairs_pd['label'].value_counts())\n",
    "\n",
    "# Check some example user contexts and book texts\n",
    "print(\"\\nExample user context:\")\n",
    "print(training_pairs_pd['user_ctx'].iloc[0])\n",
    "print(\"\\nExample book text:\")\n",
    "print(training_pairs_pd['book_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of positive examples per user:\n",
      "user_id\n",
      "0005f52944ea1992e95d61f287acaea9     65\n",
      "0006260f85929db85eddee3a0bd0e504     20\n",
      "0006db397ebf02b2e891d1048fb70dbc    166\n",
      "0006de2967df1ec4432c51090803966e     76\n",
      "000883382802f2d95a3dd545bb953882    154\n",
      "                                   ... \n",
      "1d364492146d00ceebf9b7ec4e7d45af    298\n",
      "1d4a2185b490d26a3ab0faedc4adf6c7     42\n",
      "1d6a5b005de5e4c27945dca1c13d47f2    121\n",
      "1de44842d3080ec55181e46b0cf16ed1     78\n",
      "1dfed2c58f01fe899666bd9a6ce319e1    155\n",
      "Length: 5357, dtype: int64\n",
      "\n",
      "Statistics about positive examples per user:\n",
      "count    5357.000000\n",
      "mean      136.083816\n",
      "std       101.274552\n",
      "min         1.000000\n",
      "25%        77.000000\n",
      "50%       109.000000\n",
      "75%       165.000000\n",
      "max      1478.000000\n",
      "dtype: float64\n",
      "\n",
      "Number of users with less than 5 positive examples: 30\n",
      "\n",
      "Example users with few positives:\n",
      "user_id\n",
      "008c374625966c32477ebab37e835a4e    1\n",
      "00e8157279aa30f4b919aea0a887f49a    2\n",
      "01e2d286d0361edf8c62bc580d3baa18    1\n",
      "02ac01d9ebc7165e80d8967f075adbd3    3\n",
      "0378ae8905e15ae09e01cad7c307a78b    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count positive labels per user\n",
    "positive_counts = training_pairs_df[training_pairs_df['label'] == 1].groupby('user_id').size()\n",
    "print(\"\\nNumber of positive examples per user:\")\n",
    "print(positive_counts)\n",
    "\n",
    "# Get some statistics about the positive counts\n",
    "print(\"\\nStatistics about positive examples per user:\")\n",
    "print(positive_counts.describe())\n",
    "\n",
    "# Find users with very few positive examples\n",
    "users_with_few_positives = positive_counts[positive_counts < 5]\n",
    "print(f\"\\nNumber of users with less than 5 positive examples: {len(users_with_few_positives)}\")\n",
    "print(\"\\nExample users with few positives:\")\n",
    "print(users_with_few_positives.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ArrowStringArray>\n",
      "['001af7947e217e17694c5a9c097afffb', '0006260f85929db85eddee3a0bd0e504',\n",
      " '000bcda59ab565512f51f9e1f531b5e5', '0005f52944ea1992e95d61f287acaea9',\n",
      " '000883382802f2d95a3dd545bb953882', '0006db397ebf02b2e891d1048fb70dbc',\n",
      " '0009b61b9879bb2e5b84ce24f43450c8', '00281bdc3b8dd584ca6c5cb867de959f',\n",
      " '0006de2967df1ec4432c51090803966e', '002c10ebc541a4303b4d2c0aa2bff335',\n",
      " ...\n",
      " '090ebce33f677e84d5ee0e8510996a15', '093a06eb1563ef6d2a6c443b5189db47',\n",
      " '0a97f788f5707a7f116f5cc16875597e', '0951f343eed8911a4451ae2fa80dc1f3',\n",
      " '08c70632f3c4ca2793d221f9d47037fb', '096eb5757df185c7793fac23085a5b62',\n",
      " '08c0a7ae8992a65d7792dd9c69b41369', '090fd9cea80dbfb06cf11885af8a1e38',\n",
      " '093ef1f64c3baa83e54d9e63a550369d', '089c7ad67ccf2f81c8dc476db53f3235']\n",
      "Length: 1785, dtype: string\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Get all unique user IDs\n",
    "user_ids = training_pairs_pd['user_id'].unique()\n",
    "\n",
    "print(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (2862473, 5)\n",
      "Number of positive samples after filtering/sampling: 16017\n",
      "Number of unique users after filtering/sampling positives: 5339\n",
      "Final processed DataFrame shape: (64068, 5)\n",
      "Label distribution in final DataFrame:\n",
      "label\n",
      "0    0.75\n",
      "1    0.25\n",
      "Name: proportion, dtype: float64\n",
      "Number of unique users in final DataFrame: 5339\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Assuming 'df' is your DataFrame loaded with pd.read_parquet(data_path)\n",
    "# and has columns: 'user_id', 'label', 'user_ctx', 'book_text'\n",
    "\n",
    "# --- Configuration ---\n",
    "MIN_POSITIVES_TO_KEEP_USER = 3\n",
    "MAX_POSITIVES_TO_SAMPLE = 3  # Changed from 10 to 3\n",
    "NEGATIVES_PER_POSITIVE = 3\n",
    "RANDOM_SEED = 42 # For reproducibility\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "print(f\"Original DataFrame shape: {training_pairs_df.shape}\")\n",
    "\n",
    "# --- Step 1: Identify positive and negative interactions ---\n",
    "df_positives = training_pairs_df[training_pairs_df['label'] == 1]\n",
    "df_negatives = training_pairs_df[training_pairs_df['label'] == 0]\n",
    "\n",
    "# --- Step 2: Process each user ---\n",
    "selected_positive_samples = []\n",
    "users_to_process = df_positives['user_id'].unique()\n",
    "\n",
    "for user_id in users_to_process:\n",
    "    user_positive_df = df_positives[df_positives['user_id'] == user_id]\n",
    "    num_user_positives = len(user_positive_df)\n",
    "\n",
    "    if num_user_positives >= MAX_POSITIVES_TO_SAMPLE:\n",
    "        # If >= 3 positives, sample 3\n",
    "        selected_positive_samples.append(user_positive_df.sample(n=MAX_POSITIVES_TO_SAMPLE, random_state=RANDOM_SEED))\n",
    "    elif num_user_positives >= MIN_POSITIVES_TO_KEEP_USER:\n",
    "        # If exactly 3 positives, keep them all\n",
    "        selected_positive_samples.append(user_positive_df)\n",
    "    # Else (less than 3 positives), drop the user (do nothing here)\n",
    "\n",
    "# Combine all selected positive samples\n",
    "if selected_positive_samples:\n",
    "    final_positives_df = pd.concat(selected_positive_samples).reset_index(drop=True)\n",
    "else:\n",
    "    final_positives_df = pd.DataFrame(columns=df.columns) # Empty DataFrame if no users meet criteria\n",
    "\n",
    "print(f\"Number of positive samples after filtering/sampling: {len(final_positives_df)}\")\n",
    "print(f\"Number of unique users after filtering/sampling positives: {final_positives_df['user_id'].nunique()}\")\n",
    "\n",
    "# --- Step 3: Sample negatives for each selected positive ---\n",
    "final_samples_list = []\n",
    "if not final_positives_df.empty:\n",
    "    for _, positive_row in final_positives_df.iterrows():\n",
    "        user_id = positive_row['user_id']\n",
    "        \n",
    "        # Add the positive sample\n",
    "        final_samples_list.append(positive_row.to_dict())\n",
    "        \n",
    "        # Get all negative samples for this user\n",
    "        user_negative_df = df_negatives[df_negatives['user_id'] == user_id]\n",
    "        \n",
    "        if not user_negative_df.empty:\n",
    "            num_negs_to_sample = min(NEGATIVES_PER_POSITIVE, len(user_negative_df))\n",
    "            if num_negs_to_sample > 0:\n",
    "                sampled_negatives = user_negative_df.sample(n=num_negs_to_sample, random_state=RANDOM_SEED)\n",
    "                for _, neg_row in sampled_negatives.iterrows():\n",
    "                    final_samples_list.append(neg_row.to_dict())\n",
    "\n",
    "# Create the final DataFrame\n",
    "processed_df = pd.DataFrame(final_samples_list)\n",
    "\n",
    "if not processed_df.empty:\n",
    "    # Ensure correct dtypes, especially for label\n",
    "    processed_df['label'] = processed_df['label'].astype(int)\n",
    "    print(f\"Final processed DataFrame shape: {processed_df.shape}\")\n",
    "    print(f\"Label distribution in final DataFrame:\\n{processed_df['label'].value_counts(normalize=True)}\")\n",
    "    print(f\"Number of unique users in final DataFrame: {processed_df['user_id'].nunique()}\")\n",
    "else:\n",
    "    print(\"No samples met the criteria. The processed DataFrame is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = 'data/processed_training_pairs_parts_0_to_12.parquet'\n",
    "output_model_dir = 'reranker_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 222360, Val pairs: 27792, Test pairs: 27804\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Load all pairs and drop book_id\n",
    "df = pd.read_parquet(data_path)\n",
    "df = df.drop('book_id', axis=1)\n",
    "\n",
    "# 2) Get unique users\n",
    "users = df['user_id'].unique()\n",
    "\n",
    "# 3) First split: 80% train, 20% temp (val+test)\n",
    "train_users, temp_users = train_test_split(\n",
    "    users, \n",
    "    test_size=0.2, \n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# 4) Second split: half of temp → val (10%), half → test (10%)\n",
    "val_users, test_users = train_test_split(\n",
    "    temp_users, \n",
    "    test_size=0.5, \n",
    "    random_state=random_seed\n",
    ")\n",
    "\n",
    "# 5) Build DataFrames\n",
    "train_df = df[df['user_id'].isin(train_users)].reset_index(drop=True)\n",
    "val_df   = df[df['user_id'].isin(val_users)].reset_index(drop=True)\n",
    "test_df  = df[df['user_id'].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train pairs: {len(train_df)}, Val pairs: {len(val_df)}, Test pairs: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_ctx</th>\n",
       "      <th>book_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Down and Out in Paris and London | Genr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Virgin Suicides | Genres: coming-of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Seducing Cinderella (Fighting for Love,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Highlander Untamed (MacLeods of Skye Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: A Woman in Berlin: Eight Weeks in the C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  \\\n",
       "0  001af7947e217e17694c5a9c097afffb   \n",
       "1  001af7947e217e17694c5a9c097afffb   \n",
       "2  001af7947e217e17694c5a9c097afffb   \n",
       "3  001af7947e217e17694c5a9c097afffb   \n",
       "4  001af7947e217e17694c5a9c097afffb   \n",
       "\n",
       "                                            user_ctx  \\\n",
       "0  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "1  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "2  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "3  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "4  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "\n",
       "                                           book_text  label  \n",
       "0  Title: Down and Out in Paris and London | Genr...      1  \n",
       "1  Title: The Virgin Suicides | Genres: coming-of...      0  \n",
       "2  Title: Seducing Cinderella (Fighting for Love,...      0  \n",
       "3  Title: Highlander Untamed (MacLeods of Skye Tr...      0  \n",
       "4  Title: A Woman in Berlin: Eight Weeks in the C...      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare InputExamples & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to InputExample\n",
    "train_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in train_df.itertuples()\n",
    "]\n",
    "\n",
    "val_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in val_df.itertuples()\n",
    "]\n",
    "\n",
    "test_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in test_df.itertuples()\n",
    "]\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader   = DataLoader(val_examples, shuffle=False, batch_size=batch_size)\n",
    "test_dataloader  = DataLoader(test_examples, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Pre Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Baseline Model: Pre-trained CrossEncoder ---\n",
      "Using test_df with shape: (27804, 4) for pre-trained baseline evaluation.\n",
      "Successfully loaded pre-trained model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Making predictions with the pre-trained model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 869/869 [02:33<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pre-trained CrossEncoder Baseline Results ---\n",
      "Global ROC AUC: 0.5988\n",
      "Global Average Precision (across all items): 0.4133\n",
      "Mean Average Precision (MAP): 0.5921\n",
      "Mean Per-User NDCG@3: 0.4806\n",
      "Mean Per-User NDCG@5: 0.5731\n",
      "Mean Per-User NDCG@10: 0.6940\n",
      "--- Baseline Model: Pre-trained CrossEncoder Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Baseline Model: Pre-trained CrossEncoder (No Fine-Tuning) ---\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Starting Baseline Model: Pre-trained CrossEncoder ---\")\n",
    "\n",
    "# Initialize metric variables for baseline\n",
    "baseline_global_roc_auc = None\n",
    "baseline_global_ap = None\n",
    "baseline_map = None\n",
    "baseline_ndcg_3 = None\n",
    "baseline_ndcg_5 = None\n",
    "baseline_ndcg_10 = None\n",
    "\n",
    "# --- Configuration ---\n",
    "PRETRAINED_MODEL_NAME = 'cross-encoder/ms-marco-MiniLM-L-6-v2' \n",
    "MAX_LENGTH_PRETRAINED = 256\n",
    "\n",
    "if 'test_df' not in locals() or not isinstance(test_df, pd.DataFrame) or test_df.empty:\n",
    "    print(\"Error: `test_df` is not defined, not a DataFrame, or is empty.\")\n",
    "    print(\"Please ensure `test_df` is created and populated from your data splitting cells before running this baseline.\")\n",
    "else:\n",
    "    print(f\"Using test_df with shape: {test_df.shape} for pre-trained baseline evaluation.\")\n",
    "    baseline_model_instance = None # Renamed to avoid conflict if 'baseline_model' is used elsewhere\n",
    "    try:\n",
    "        baseline_model_instance = CrossEncoder(\n",
    "            PRETRAINED_MODEL_NAME,\n",
    "            max_length=MAX_LENGTH_PRETRAINED\n",
    "        )\n",
    "        print(f\"Successfully loaded pre-trained model: {PRETRAINED_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading pre-trained model {PRETRAINED_MODEL_NAME}: {e}\")\n",
    "\n",
    "    if baseline_model_instance:\n",
    "        if 'user_ctx' not in test_df.columns or 'book_text' not in test_df.columns:\n",
    "            print(\"Error: `test_df` is missing 'user_ctx' or 'book_text' columns.\")\n",
    "        else:\n",
    "            test_pairs_for_baseline = [[row.user_ctx, row.book_text] for row in test_df.itertuples()]\n",
    "            \n",
    "            if not test_pairs_for_baseline:\n",
    "                print(\"Error: `test_pairs_for_baseline` list is empty. Cannot make predictions.\")\n",
    "            else:\n",
    "                print(\"Making predictions with the pre-trained model...\")\n",
    "                baseline_scores_pred = [] \n",
    "                try:\n",
    "                    baseline_scores_pred = baseline_model_instance.predict(test_pairs_for_baseline, show_progress_bar=True)\n",
    "                    \n",
    "                    if 'label' not in test_df.columns:\n",
    "                        print(\"Error: `test_df` is missing 'label' column for evaluation.\")\n",
    "                    else:\n",
    "                        test_true_labels = test_df['label'].values\n",
    "\n",
    "                        if len(baseline_scores_pred) == len(test_true_labels):\n",
    "                            print(f\"\\n--- Pre-trained CrossEncoder Baseline Results ---\")\n",
    "                            \n",
    "                            baseline_global_roc_auc = roc_auc_score(test_true_labels, baseline_scores_pred)\n",
    "                            baseline_global_ap = average_precision_score(test_true_labels, baseline_scores_pred)\n",
    "                            print(f\"Global ROC AUC: {baseline_global_roc_auc:.4f}\")\n",
    "                            print(f\"Global Average Precision (across all items): {baseline_global_ap:.4f}\")\n",
    "\n",
    "                            user_ndcg_scores_at_3_bl = [] # Suffix _bl for baseline\n",
    "                            user_ndcg_scores_at_5_bl = []\n",
    "                            user_ndcg_scores_at_10_bl = []\n",
    "                            user_ap_scores_bl = []\n",
    "\n",
    "                            if 'user_id' in test_df.columns:\n",
    "                                evaluation_df_baseline = pd.DataFrame({\n",
    "                                    'user_id': test_df['user_id'], \n",
    "                                    'label': test_true_labels,      \n",
    "                                    'score': baseline_scores_pred \n",
    "                                })\n",
    "\n",
    "                                for user_id_val in evaluation_df_baseline['user_id'].unique():\n",
    "                                    user_data = evaluation_df_baseline[evaluation_df_baseline['user_id'] == user_id_val]\n",
    "                                    \n",
    "                                    if not user_data.empty and user_data['label'].sum() > 0:\n",
    "                                        y_true_user_list = user_data['label'].values.tolist()\n",
    "                                        y_score_user_list = user_data['score'].values.tolist()\n",
    "\n",
    "                                        user_ap = average_precision_score(y_true_user_list, y_score_user_list)\n",
    "                                        user_ap_scores_bl.append(user_ap)\n",
    "                                        \n",
    "                                        y_true_user_ndcg = [y_true_user_list]\n",
    "                                        y_score_user_ndcg = [y_score_user_list]\n",
    "                                        \n",
    "                                        k3 = min(3, len(user_data))\n",
    "                                        if k3 > 0:\n",
    "                                            user_ndcg_3_val = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k3) # temp var\n",
    "                                            user_ndcg_scores_at_3_bl.append(user_ndcg_3_val)\n",
    "                                        \n",
    "                                        k5 = min(5, len(user_data))\n",
    "                                        if k5 > 0:\n",
    "                                            user_ndcg_5_val = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k5) # temp var\n",
    "                                            user_ndcg_scores_at_5_bl.append(user_ndcg_5_val)\n",
    "\n",
    "                                        k10 = min(10, len(user_data))\n",
    "                                        if k10 > 0:\n",
    "                                             user_ndcg_10_val = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k10) # temp var\n",
    "                                             user_ndcg_scores_at_10_bl.append(user_ndcg_10_val)\n",
    "                                \n",
    "                                if user_ap_scores_bl:\n",
    "                                    baseline_map = np.mean(user_ap_scores_bl)\n",
    "                                    print(f\"Mean Average Precision (MAP): {baseline_map:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"MAP could not be calculated (no valid user AP scores).\")\n",
    "\n",
    "                                if user_ndcg_scores_at_3_bl:\n",
    "                                    baseline_ndcg_3 = np.mean(user_ndcg_scores_at_3_bl)\n",
    "                                    print(f\"Mean Per-User NDCG@3: {baseline_ndcg_3:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"Mean Per-User NDCG@3 could not be calculated.\")\n",
    "                                \n",
    "                                if user_ndcg_scores_at_5_bl:\n",
    "                                    baseline_ndcg_5 = np.mean(user_ndcg_scores_at_5_bl)\n",
    "                                    print(f\"Mean Per-User NDCG@5: {baseline_ndcg_5:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"Mean Per-User NDCG@5 could not be calculated.\")\n",
    "\n",
    "                                if user_ndcg_scores_at_10_bl: \n",
    "                                    baseline_ndcg_10 = np.mean(user_ndcg_scores_at_10_bl)\n",
    "                                    print(f\"Mean Per-User NDCG@10: {baseline_ndcg_10:.4f}\")\n",
    "                                else:\n",
    "                                    print(\"Mean Per-User NDCG@10 could not be calculated.\")\n",
    "                            else:\n",
    "                                print(\"Error: 'user_id' column not found in test_df. Cannot calculate per-user metrics.\")\n",
    "                        else:\n",
    "                            print(f\"Error: Mismatch in length between predicted scores ({len(baseline_scores_pred)}) and true labels ({len(test_true_labels)}). Evaluation skipped.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during prediction or evaluation with pre-trained model: {e}\")\n",
    "\n",
    "print(\"--- Baseline Model: Pre-trained CrossEncoder Finished ---\")\n",
    "\n",
    "# Verification print for baseline metrics\n",
    "print(\"\\n--- Saved Metrics for Baseline Model (for later comparison) ---\")\n",
    "print(f\"baseline_global_roc_auc: {baseline_global_roc_auc}\")\n",
    "print(f\"baseline_global_ap: {baseline_global_ap}\")\n",
    "print(f\"baseline_map: {baseline_map}\")\n",
    "print(f\"baseline_ndcg_3: {baseline_ndcg_3}\")\n",
    "print(f\"baseline_ndcg_5: {baseline_ndcg_5}\")\n",
    "print(f\"baseline_ndcg_10: {baseline_ndcg_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global ROC AUC: 0.5988\n",
    "Interpretation: Still indicates a modest ability to globally distinguish positive from negative pairs, slightly better than random. This hasn't changed from the previous interpretation.\n",
    "Global Average Precision (across all items): 0.4133\n",
    "Interpretation: The model has some capability in ranking positive items higher than negative items when all user-item pairs are considered together. Again, better than a random baseline (which might be around 0.25 globally given your 1:3 pos/neg ratio within users).\n",
    "Mean Average Precision (MAP): 0.5921\n",
    "Interpretation: This is a strong indicator of per-user ranking quality. A MAP of ~0.59 is quite good for a pre-trained baseline. It means that, on average, when you look at the ranked list for each user, the precision is maintained reasonably well as you go down the list to find all relevant items. It's a more robust measure of overall per-user ranking than just looking at one point (like P@k).\n",
    "Comparison to Global AP: Notice that MAP (0.5921) is significantly higher than the Global AP (0.4133). This is common and important. It suggests that while the model might struggle a bit when all items are jumbled, its performance within each user's individual list of 12 candidates is notably better. This is exactly what you want for a recommendation reranker.\n",
    "Mean Per-User NDCG@3: 0.4806\n",
    "Interpretation: This tells you about the quality of the ranking specifically for the top 3 positions. Since each user has exactly 3 positive items, this metric is crucial.\n",
    "An NDCG@3 of ~0.48 means that, on average, the model is doing a moderately good job of getting the 3 positive items into the top 3 slots, but it's far from perfect. If it were perfect for every user (all 3 positives in the top 3), NDCG@3 would be 1.0.\n",
    "This score suggests that often, one or more of the positive items might be ranked below position 3, or some negative items are intruding into the top 3.\n",
    "Mean Per-User NDCG@5: 0.5731\n",
    "Interpretation: Looking at the top 5 positions, the ranking quality improves. This means that even if not all 3 positive items make it into the top 3, they are often found within the top 5.\n",
    "The increase from NDCG@3 (0.4806) to NDCG@5 (0.5731) is logical and expected.\n",
    "Mean Per-User NDCG@10: 0.6940\n",
    "Interpretation: This remains a good score. By considering the top 10 out of 12 items, the model has more opportunity to place all 3 positive items correctly, and this score reflects that. The jump from NDCG@5 to NDCG@10 is also substantial, indicating that many of the relevant items that weren't in the top 5 are indeed captured within the top 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Validation Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import CrossEncoderEvaluator\n",
    "\n",
    "evaluator = CrossEncoderEvaluator.from_input_examples(\n",
    "    val_examples,     \n",
    "    name='val',\n",
    "    batch_size=16,\n",
    "    main_score_function=lambda y_true, y_pred: ndcg_score([y_true], [y_pred], k=3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate & Fine-Tune CrossEncoder\n",
    "\n",
    "cross-encoder/ms-marco-MiniLM-L-6-v2 is a 6-layer MiniLM distilled into a cross-encoder architecture and pretrained on the MS MARCO passage ranking task. It takes a paired input (e.g. user context + book text) and produces a single relevance score via full token-level attention.\n",
    "\n",
    "Justification\n",
    "\t•\tRanking-Tuned Pretraining\n",
    "Its MS MARCO heritage means it already knows how to judge fine-grained relevance patterns—crucial for matching nuanced book descriptions to user tastes.\n",
    "\t•\tSpeed-Quality Sweet Spot\n",
    "At ~60 MB and with inference under 15 ms per candidate, it delivers ~90–95 % of full BERT-base accuracy, keeping end-to-end latency low.\n",
    "\t•\tEfficient Fine-Tuning\n",
    "Requires only 2–3 epochs over ~150 K (user,book) pairs to adapt deeply to book-domain language, making rapid iteration feasible.\n",
    "\t•\tCompact & Deployable\n",
    "Its small footprint simplifies packaging, loading, and scaling in production environments with moderate memory and compute budgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training steps per epoch: 13898\n",
      "Calculated warmup steps (10% of one epoch): 1389\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# 1. Calculate the number of training steps per epoch\n",
    "num_train_steps_per_epoch = len(train_dataloader)\n",
    "print(f\"Number of training steps per epoch: {num_train_steps_per_epoch}\")\n",
    "\n",
    "# 2. Calculate warmup steps\n",
    "warmup_ratio = 0.10  # 10% for warmup\n",
    "actual_warmup_steps = int(num_train_steps_per_epoch * warmup_ratio)\n",
    "print(f\"Calculated warmup steps (10% of one epoch): {actual_warmup_steps}\")\n",
    "\n",
    "# Ensure warmup_steps is not zero if num_train_steps_per_epoch is very small\n",
    "if actual_warmup_steps == 0 and num_train_steps_per_epoch > 0:\n",
    "    actual_warmup_steps = 1 # Ensure at least 1 step if training is happening\n",
    "    print(f\"Adjusted warmup steps to 1 as calculated value was 0.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "model = CrossEncoder(\n",
    "    'cross-encoder/ms-marco-MiniLM-L-6-v2',\n",
    "    num_labels=1,\n",
    "    max_length=384,  \n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    evaluator=evaluator,\n",
    "    epochs=epochs, # e.g., 3\n",
    "    learning_rate=learning_rate, # e.g., 2e-5\n",
    "    scheduler='WarmupLinear', \n",
    "    warmup_steps=actual_warmup_steps, \n",
    "    evaluation_steps=num_train_steps_per_epoch,  \n",
    "    output_path=output_model_dir,\n",
    "    save_best_model=True,\n",
    "    use_amp=True,\n",
    "    early_stopping=True,\n",
    "    early_stopping_patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # Make sure this is imported\n",
    "\n",
    "# --- Plotting Validation Performance for CrossEncoder ---\n",
    "print(\"\\n--- Plotting CrossEncoder Validation Performance ---\")\n",
    "\n",
    "# Define where your model.fit() saved its output\n",
    "# This should be the same as the 'output_path' you gave to model.fit()\n",
    "output_model_dir_from_training = output_model_dir # Or directly use 'reranker_model' if that's the variable\n",
    "\n",
    "# The evaluator was named 'val' in your setup\n",
    "evaluator_name = 'val' \n",
    "eval_filename = f\"{evaluator_name}_results.csv\"\n",
    "\n",
    "# Path to the evaluation results CSV file\n",
    "# It's typically directly in the output_path for CrossEncoder, or sometimes in an 'eval' subfolder.\n",
    "# Check your output_model_dir after training to confirm the exact path.\n",
    "# Common paths:\n",
    "# 1. output_model_dir / evaluator_name_results.csv\n",
    "# 2. output_model_dir / eval / evaluator_name_results.csv\n",
    "\n",
    "# Let's try the first common path:\n",
    "eval_filepath = os.path.join(output_model_dir_from_training, eval_filename)\n",
    "\n",
    "# If not found, try the 'eval' subdirectory (less common for CrossEncoder's default save)\n",
    "if not os.path.exists(eval_filepath):\n",
    "    eval_filepath_alt = os.path.join(output_model_dir_from_training, \"eval\", eval_filename)\n",
    "    if os.path.exists(eval_filepath_alt):\n",
    "        eval_filepath = eval_filepath_alt\n",
    "    else:\n",
    "        print(f\"Could not find evaluation file at {eval_filepath} or {eval_filepath_alt}\")\n",
    "        # Set eval_filepath to a non-existent path to trigger FileNotFoundError below if needed\n",
    "        eval_filepath = os.path.join(output_model_dir_from_training, \"FILE_DOES_NOT_EXIST.csv\")\n",
    "\n",
    "\n",
    "# The main score column in the CSV will match the metric from your main_score_function\n",
    "# Your main_score_function was ndcg_score with k=3.\n",
    "# The column name is often the name of the evaluator + \"_\" + the metric name (or just the metric name).\n",
    "# For NDCG from ndcg_score, it might be saved as 'ndcg' or 'score', or 'val_ndcg'.\n",
    "# You'll need to INSPECT the CSV file after one training run to get the exact column name.\n",
    "# Let's assume for now it's called 'score' (as it's the output of main_score_function) or 'ndcg'.\n",
    "\n",
    "# Common potential column names for the score from your main_score_function\n",
    "# The CrossEncoderEvaluator often names the column after the evaluator's name plus the metric.\n",
    "# If main_score_function just returns a score, it might be 'score' or based on the lambda.\n",
    "# Let's try a few common ones, or you can inspect the CSV header.\n",
    "potential_score_column_names = [\n",
    "    f'{evaluator_name}_ndcg', # e.g., val_ndcg if ndcg_score returns a dict key\n",
    "    'score',                 # Generic name if the main_score_function directly returns the score\n",
    "    'ndcg',                  # If the metric itself is named 'ndcg'\n",
    "    # Add other possibilities based on inspecting your CSV file\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "    eval_results_df = pd.read_csv(eval_filepath)\n",
    "    print(f\"Successfully loaded evaluation results from: {eval_filepath}\")\n",
    "    print(\"Columns in evaluation CSV:\", eval_results_df.columns.tolist())\n",
    "\n",
    "    # Determine the actual score column name\n",
    "    score_column = None\n",
    "    for col_name in potential_score_column_names:\n",
    "        if col_name in eval_results_df.columns:\n",
    "            score_column = col_name\n",
    "            break\n",
    "    \n",
    "    if 'epoch' in eval_results_df.columns and 'steps' in eval_results_df.columns and score_column:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Plot score vs steps\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(eval_results_df['steps'], eval_results_df[score_column], marker='o', linestyle='-')\n",
    "        plt.title(f'Validation {score_column.replace(\"_\", \" \").title()} vs. Training Steps')\n",
    "        plt.xlabel('Training Steps')\n",
    "        plt.ylabel(score_column.replace(\"_\", \" \").title())\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Plot score vs epoch\n",
    "        plt.subplot(1, 2, 2)\n",
    "        epoch_end_evals = eval_results_df.loc[eval_results_df.groupby('epoch')['steps'].idxmax()]\n",
    "        plt.plot(epoch_end_evals['epoch'], epoch_end_evals[score_column], marker='o', linestyle='-')\n",
    "        plt.title(f'Validation {score_column.replace(\"_\", \" \").title()} vs. Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(score_column.replace(\"_\", \" \").title())\n",
    "        plt.xticks(epoch_end_evals['epoch'].unique()) \n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        # Ensure output_model_dir_from_training is defined correctly\n",
    "        plot_save_path = os.path.join(output_model_dir_from_training, f'{evaluator_name}_performance_plot.png')\n",
    "        plt.savefig(plot_save_path)\n",
    "        print(f\"Validation plot saved to: {plot_save_path}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        missing_cols = []\n",
    "        if 'epoch' not in eval_results_df.columns: missing_cols.append('epoch')\n",
    "        if 'steps' not in eval_results_df.columns: missing_cols.append('steps')\n",
    "        if not score_column: missing_cols.append(f\"one of {potential_score_column_names}\")\n",
    "        print(f\"Required columns {missing_cols} not found in {eval_filepath}. Cannot plot.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Evaluation results file not found. Checked paths ending with {eval_filename}\")\n",
    "    print(\"Plotting skipped. Ensure `output_path` was set in `model.fit()` and training completed at least one evaluation step.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during plotting: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, ndcg_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n--- Evaluating Fine-Tuned CrossEncoder Model on Test Set ---\")\n",
    "\n",
    "# Initialize metric variables to None or a default (e.g., 0.0) in case evaluation fails\n",
    "finetuned_global_roc_auc = None\n",
    "finetuned_global_ap = None\n",
    "finetuned_map = None\n",
    "finetuned_ndcg_3 = None\n",
    "finetuned_ndcg_5 = None\n",
    "finetuned_ndcg_10 = None\n",
    "\n",
    "if 'test_df' not in locals() or not isinstance(test_df, pd.DataFrame) or test_df.empty:\n",
    "    print(\"Error: `test_df` is not defined, not a DataFrame, or is empty.\")\n",
    "    print(\"Please ensure `test_df` is available for final model evaluation.\")\n",
    "else:\n",
    "    if 'user_ctx' not in test_df.columns or 'book_text' not in test_df.columns or 'label' not in test_df.columns:\n",
    "        print(\"Error: `test_df` is missing one or more required columns: 'user_ctx', 'book_text', 'label'.\")\n",
    "    else:\n",
    "        # 1. Prepare test pairs for prediction\n",
    "        test_pairs_for_model = [[row.user_ctx, row.book_text] for row in test_df.itertuples()]\n",
    "        \n",
    "        if not test_pairs_for_model:\n",
    "            print(\"Error: `test_pairs_for_model` list is empty. Cannot make predictions.\")\n",
    "        else:\n",
    "            print(\"Making predictions with the fine-tuned model...\")\n",
    "            # 2. Get Predictions from the fine-tuned model\n",
    "            # Make sure your 'model' variable refers to the trained model instance\n",
    "            try:\n",
    "                model_scores = model.predict(test_pairs_for_model, show_progress_bar=True)\n",
    "                true_labels = test_df['label'].values\n",
    "\n",
    "                if len(model_scores) == len(true_labels):\n",
    "                    print(f\"\\n--- Fine-Tuned Model Test Set Results ---\")\n",
    "\n",
    "                    # 3. Compute and Store Global Metrics\n",
    "                    finetuned_global_roc_auc = roc_auc_score(true_labels, model_scores)\n",
    "                    finetuned_global_ap = average_precision_score(true_labels, model_scores)\n",
    "                    print(f\"Global ROC AUC: {finetuned_global_roc_auc:.4f}\")\n",
    "                    print(f\"Global Average Precision (across all items): {finetuned_global_ap:.4f}\")\n",
    "\n",
    "                    # 4. Compute and Store Per-User Metrics (MAP, NDCG@3, NDCG@5, NDCG@10)\n",
    "                    user_ap_scores_ft = []\n",
    "                    user_ndcg_scores_at_3_ft = []\n",
    "                    user_ndcg_scores_at_5_ft = []\n",
    "                    user_ndcg_scores_at_10_ft = []\n",
    "\n",
    "                    if 'user_id' in test_df.columns:\n",
    "                        evaluation_df_ft = pd.DataFrame({\n",
    "                            'user_id': test_df['user_id'],\n",
    "                            'label': true_labels,\n",
    "                            'score': model_scores\n",
    "                        })\n",
    "\n",
    "                        for user_id_val in evaluation_df_ft['user_id'].unique():\n",
    "                            user_data = evaluation_df_ft[evaluation_df_ft['user_id'] == user_id_val]\n",
    "\n",
    "                            if not user_data.empty and user_data['label'].sum() > 0:\n",
    "                                y_true_user_list = user_data['label'].values.tolist()\n",
    "                                y_score_user_list = user_data['score'].values.tolist()\n",
    "\n",
    "                                user_ap = average_precision_score(y_true_user_list, y_score_user_list)\n",
    "                                user_ap_scores_ft.append(user_ap)\n",
    "\n",
    "                                y_true_user_ndcg = [y_true_user_list]\n",
    "                                y_score_user_ndcg = [y_score_user_list]\n",
    "\n",
    "                                k3 = min(3, len(user_data))\n",
    "                                if k3 > 0:\n",
    "                                    user_ndcg_3 = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k3)\n",
    "                                    user_ndcg_scores_at_3_ft.append(user_ndcg_3)\n",
    "                                \n",
    "                                k5 = min(5, len(user_data))\n",
    "                                if k5 > 0:\n",
    "                                    user_ndcg_5 = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k5)\n",
    "                                    user_ndcg_scores_at_5_ft.append(user_ndcg_5)\n",
    "\n",
    "                                k10 = min(10, len(user_data))\n",
    "                                if k10 > 0:\n",
    "                                    user_ndcg_10 = ndcg_score(y_true_user_ndcg, y_score_user_ndcg, k=k10)\n",
    "                                    user_ndcg_scores_at_10_ft.append(user_ndcg_10)\n",
    "                        \n",
    "                        if user_ap_scores_ft:\n",
    "                            finetuned_map = np.mean(user_ap_scores_ft)\n",
    "                            print(f\"Mean Average Precision (MAP): {finetuned_map:.4f}\")\n",
    "                        else:\n",
    "                            print(\"MAP could not be calculated.\")\n",
    "\n",
    "                        if user_ndcg_scores_at_3_ft:\n",
    "                            finetuned_ndcg_3 = np.mean(user_ndcg_scores_at_3_ft)\n",
    "                            print(f\"Mean Per-User NDCG@3: {finetuned_ndcg_3:.4f}\")\n",
    "                        else:\n",
    "                            print(\"Mean Per-User NDCG@3 could not be calculated.\")\n",
    "                        \n",
    "                        if user_ndcg_scores_at_5_ft:\n",
    "                            finetuned_ndcg_5 = np.mean(user_ndcg_scores_at_5_ft)\n",
    "                            print(f\"Mean Per-User NDCG@5: {finetuned_ndcg_5:.4f}\")\n",
    "                        else:\n",
    "                            print(\"Mean Per-User NDCG@5 could not be calculated.\")\n",
    "\n",
    "                        if user_ndcg_scores_at_10_ft:\n",
    "                            finetuned_ndcg_10 = np.mean(user_ndcg_scores_at_10_ft)\n",
    "                            print(f\"Mean Per-User NDCG@10: {finetuned_ndcg_10:.4f}\")\n",
    "                        else:\n",
    "                            print(\"Mean Per-User NDCG@10 could not be calculated.\")\n",
    "                    else:\n",
    "                        print(\"Error: 'user_id' column not found in test_df. Cannot calculate per-user metrics.\")\n",
    "                else:\n",
    "                    print(f\"Error: Mismatch in length between model_scores ({len(model_scores)}) and true_labels ({len(true_labels)}). Evaluation skipped.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error during prediction or evaluation with the fine-tuned model: {e}\")\n",
    "\n",
    "print(\"--- Fine-Tuned Model Evaluation Finished ---\")\n",
    "\n",
    "# You can now verify the variables:\n",
    "print(\"\\n--- Saved Metrics for Fine-Tuned Model (for later comparison) ---\")\n",
    "print(f\"finetuned_global_roc_auc: {finetuned_global_roc_auc}\")\n",
    "print(f\"finetuned_global_ap: {finetuned_global_ap}\")\n",
    "print(f\"finetuned_map: {finetuned_map}\")\n",
    "print(f\"finetuned_ndcg_3: {finetuned_ndcg_3}\")\n",
    "print(f\"finetuned_ndcg_5: {finetuned_ndcg_5}\")\n",
    "print(f\"finetuned_ndcg_10: {finetuned_ndcg_10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Metrics from Baseline Model ---\n",
    "baseline_metrics = {\n",
    "    'Global ROC AUC': baseline_global_roc_auc,\n",
    "    'Global AP': baseline_global_ap,\n",
    "    'MAP': baseline_map,\n",
    "    'NDCG@3': baseline_ndcg_3,\n",
    "    'NDCG@5': baseline_ndcg_5,\n",
    "    'NDCG@10': baseline_ndcg_10\n",
    "}\n",
    "\n",
    "# --- Metrics from Fine-Tuned Model ---\n",
    "finetuned_metrics = {\n",
    "    'Global ROC AUC': finetuned_global_roc_auc,  \n",
    "    'Global AP': finetuned_global_ap,       \n",
    "    'MAP': finetuned_map,             \n",
    "    'NDCG@3': finetuned_ndcg_3,          \n",
    "    'NDCG@5': finetuned_ndcg_5,           \n",
    "    'NDCG@10': finetuned_ndcg_10         \n",
    "}\n",
    "\n",
    "# 1. Create a Comparison Table\n",
    "metrics_data = {\n",
    "    'Metric': list(baseline_metrics.keys()),\n",
    "    'Baseline': list(baseline_metrics.values()),\n",
    "    'Fine-Tuned': list(finetuned_metrics.values())\n",
    "}\n",
    "comparison_df = pd.DataFrame(metrics_data)\n",
    "comparison_df['Improvement'] = comparison_df['Fine-Tuned'] - comparison_df['Baseline']\n",
    "comparison_df['Improvement (%)'] = (comparison_df['Improvement'] / comparison_df['Baseline']) * 100\n",
    "comparison_df['Improvement (%)'] = comparison_df['Improvement (%)'].round(2)\n",
    "\n",
    "print(\"--- Model Performance Comparison ---\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "# 2. Generate Bar Charts for Visual Comparison\n",
    "metric_names = list(baseline_metrics.keys())\n",
    "baseline_values = list(baseline_metrics.values())\n",
    "finetuned_values = list(finetuned_metrics.values())\n",
    "\n",
    "num_metrics = len(metric_names)\n",
    "bar_width = 0.35\n",
    "index = np.arange(num_metrics)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "rects1 = ax.bar(index - bar_width/2, baseline_values, bar_width, label='Baseline', color='skyblue')\n",
    "rects2 = ax.bar(index + bar_width/2, finetuned_values, bar_width, label='Fine-Tuned', color='coral')\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Baseline vs. Fine-Tuned Model Performance by Metric')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(metric_names, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom',\n",
    "                    fontsize=8)\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "key_per_user_metrics = ['MAP', 'NDCG@3', 'NDCG@5', 'NDCG@10']\n",
    "key_baseline_values = [baseline_metrics[m] for m in key_per_user_metrics]\n",
    "key_finetuned_values = [finetuned_metrics[m] for m in key_per_user_metrics]\n",
    "\n",
    "num_key_metrics = len(key_per_user_metrics)\n",
    "index_key = np.arange(num_key_metrics)\n",
    "\n",
    "fig_key, ax_key = plt.subplots(figsize=(10, 6))\n",
    "rects1_key = ax_key.bar(index_key - bar_width/2, key_baseline_values, bar_width, label='Baseline', color='skyblue')\n",
    "rects2_key = ax_key.bar(index_key + bar_width/2, key_finetuned_values, bar_width, label='Fine-Tuned', color='coral')\n",
    "\n",
    "ax_key.set_ylabel('Scores')\n",
    "ax_key.set_title('Key Per-User Metrics: Baseline vs. Fine-Tuned')\n",
    "ax_key.set_xticks(index_key)\n",
    "ax_key.set_xticklabels(key_per_user_metrics)\n",
    "ax_key.legend()\n",
    "\n",
    "autolabel(rects1_key)\n",
    "autolabel(rects2_key)\n",
    "\n",
    "fig_key.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is already saved during fit\n",
    "# To load:\n",
    "from sentence_transformers import CrossEncoder\n",
    "loaded_model = CrossEncoder(output_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a single user and its candidates\n",
    "user_ctx = \"Favorite books: ...\"  # fetched or precomputed\n",
    "candidate_texts = [\"Title: ... Description: ...\", ...]\n",
    "pairs = [[user_ctx, txt] for txt in candidate_texts]\n",
    "scores = loaded_model.predict(pairs)\n",
    "\n",
    "# Rerank\n",
    "candidates = ['book1', 'book2', ...]\n",
    "ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "print(ranked[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
