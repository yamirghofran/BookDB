{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Encoder Fine-Tuning & Evaluation Notebook\n",
    "\n",
    "This notebook outlines the end-to-end process for fine-tuning a Sentence-Transformers `CrossEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sentence_transformers import CrossEncoder, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = 'data/tmodels/crossencoder_book_reranker'raining_pairs.parquet'  # your tuning pairs\n",
    "output_model_dir = '\n",
    "\n",
    "# Training parameters\n",
    "test_size = 0.1        # fraction of users for validation/test\n",
    "epochs = 2\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 100\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load & Split Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Load all pairs\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "# Split by user to avoid leak\n",
    "users = df['user_id'].unique()\n",
    "train_users, test_users = train_test_split(users, test_size=test_size, random_state=random_seed)\n",
    "\n",
    "train_df = df[df['user_id'].isin(train_users)].reset_index(drop=True)\n",
    "test_df  = df[df['user_id'].isin(test_users)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train pairs: {len(train_df)}, Test pairs: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare InputExamples & DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to InputExample\n",
    "train_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in train_df.itertuples()\n",
    "]\n",
    "val_examples = [\n",
    "    InputExample(texts=[row.user_ctx, row.book_text], label=float(row.label))\n",
    "    for row in test_df.itertuples()\n",
    "]\n",
    "\n",
    "# DataLoaders\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "val_dataloader   = DataLoader(val_examples, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate & Fine-Tune CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=1,\n",
    "    max_length=256,\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_dataloader=train_dataloader,\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    optimizer_params={'lr': learning_rate},\n",
    "    evaluation_dataloader=val_dataloader,\n",
    "    evaluation_steps=1000,\n",
    "    output_path=output_model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test pairs for prediction\n",
    "test_pairs = [[row.user_ctx, row.book_text] for row in test_df.itertuples()]\n",
    "scores = model.predict(test_pairs)\n",
    "labels = test_df['label'].values\n",
    "\n",
    "# Compute metrics\n",
    "auc = roc_auc_score(labels, scores)\n",
    "ap = average_precision_score(labels, scores)\n",
    "ndcg = ndcg_score([labels], [scores], k=10)\n",
    "\n",
    "print(f\"ROC AUC: {auc:.4f}\")\n",
    "print(f\"Average Precision: {ap:.4f}\")\n",
    "print(f\"NDCG@10: {ndcg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save & Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model is already saved during fit\n",
    "# To load:\n",
    "from sentence_transformers import CrossEncoder\n",
    "loaded_model = CrossEncoder(output_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a single user and its candidates\n",
    "user_ctx = \"Favorite books: ...\"  # fetched or precomputed\n",
    "candidate_texts = [\"Title: ... Description: ...\", ...]\n",
    "pairs = [[user_ctx, txt] for txt in candidate_texts]\n",
    "scores = loaded_model.predict(pairs)\n",
    "\n",
    "# Rerank\n",
    "candidates = ['book1', 'book2', ...]\n",
    "ranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)\n",
    "print(ranked[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
