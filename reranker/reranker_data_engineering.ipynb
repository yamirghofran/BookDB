{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "Importing required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import dask.dataframe as dd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up functions to import data from the cloud\n",
    "Introduces helper functions for interacting with an R2 cloud storage bucket: download_from_r2 to download files and list_bucket_contents to list available files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env file from: /Users/anze/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/BookDB/.env\n",
      "R2_ENDPOINT_URL successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "dotenv_path = '../.env'\n",
    "\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path)\n",
    "    print(f\"Loaded .env file from: {os.path.abspath(dotenv_path)}\")\n",
    "else:\n",
    "    # Try an alternative common path if the notebook CWD is the project root\n",
    "    dotenv_path_alt = 'BookDB/.env'\n",
    "    if os.path.exists(dotenv_path_alt):\n",
    "        load_dotenv(dotenv_path_alt)\n",
    "        print(f\"Loaded .env file from: {os.path.abspath(dotenv_path_alt)}\")\n",
    "    else:\n",
    "        print(f\".env file not found at {dotenv_path} or {dotenv_path_alt}. Please check the path.\")\n",
    "\n",
    "# Test if credentials are loaded\n",
    "R2_ENDPOINT_URL = os.getenv('R2_ENDPOINT_URL')\n",
    "if R2_ENDPOINT_URL:\n",
    "    print(\"R2_ENDPOINT_URL successfully loaded.\")\n",
    "else:\n",
    "    print(\"R2_ENDPOINT_URL not found. Check your .env file and path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_r2(object_name, local_path, bucket_name=\"bookdbio\"):\n",
    "    # ensure parent dir exists\n",
    "    parent_dir = os.path.dirname(local_path)\n",
    "    if parent_dir and not os.path.isdir(parent_dir):\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "        endpoint_url = os.getenv('R2_ENDPOINT_URL'),\n",
    "        aws_access_key_id = os.getenv('R2_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key = os.getenv('R2_SECRET_ACCESS_KEY'),\n",
    "        config = Config(signature_version='s3v4')\n",
    "   )\n",
    "\n",
    "    try:\n",
    "        s3.download_file(bucket_name, object_name, local_path)\n",
    "        print(f\"Successfully downloaded {object_name} to {local_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed for {object_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket_name=\"bookdbio\"):\n",
    "    \"\"\"List all objects in the R2 bucket\"\"\"\n",
    "    s3 = boto3.client('s3',\n",
    "        endpoint_url = os.getenv('R2_ENDPOINT_URL'),\n",
    "        aws_access_key_id = os.getenv('R2_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key = os.getenv('R2_SECRET_ACCESS_KEY'),\n",
    "        config = Config(signature_version='s3v4')\n",
    "   )\n",
    "    \n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            print(\"Available files in bucket:\")\n",
    "            for obj in response['Contents']:\n",
    "                print(f\"- {obj['Key']}\")\n",
    "        else:\n",
    "            print(\"Bucket is empty\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing bucket contents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_r2(files, bucket_name):\n",
    "    \"\"\"\n",
    "    Upload multiple files to Cloudflare R2 bucket\n",
    "    \n",
    "    Args:\n",
    "        files (list): List of tuples containing (file_path, object_name)\n",
    "        bucket_name (str): R2 bucket name\n",
    "    \"\"\"\n",
    "    # Configure R2 client\n",
    "    s3 = boto3.client('s3',\n",
    "        endpoint_url = os.getenv('R2_ENDPOINT_URL'),\n",
    "        aws_access_key_id = os.getenv('R2_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key = os.getenv('R2_SECRET_ACCESS_KEY'),\n",
    "        config = Config(signature_version='s3v4')\n",
    "   )\n",
    "\n",
    "    for file_path, object_name in files:\n",
    "        try:\n",
    "            s3.upload_file(file_path, bucket_name, object_name)\n",
    "            print(f\"Successfully uploaded {file_path} to {object_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Upload failed for {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files in bucket:\n",
      "- data/author_id_map.csv\n",
      "- data/authors.parquet\n",
      "- data/book_id_map.csv\n",
      "- data/book_texts.parquet\n",
      "- data/book_texts_reduced.parquet\n",
      "- data/books.parquet\n",
      "- data/books_dedup.parquet\n",
      "- data/books_triplets.parquet\n",
      "- data/books_triplets_reduced.parquet\n",
      "- data/books_works.parquet\n",
      "- data/interactions.parquet\n",
      "- data/interactions_dedup.parquet\n",
      "- data/interactions_prepared_ncf.parquet\n",
      "- data/interactions_prepared_ncf_reduced.parquet\n",
      "- data/item_id_map_reduced.csv\n",
      "- data/new_authors.parquet\n",
      "- data/new_books.parquet\n",
      "- data/processed_training_pairs_parts_0_to_12.parquet\n",
      "- data/reduce_books_df\n",
      "- data/reduced_book_ids.csv\n",
      "- data/reduced_books.parquet\n",
      "- data/reduced_interactions.parquet\n",
      "- data/reduced_reviews.parquet\n",
      "- data/reduced_user_ids.csv\n",
      "- data/reviews_dedup.parquet\n",
      "- data/sampled_users_book.parquet\n",
      "- data/training_pairs.parquet.zip\n",
      "- data/user_id_map.csv\n",
      "- data/user_id_map_reduced.csv\n",
      "- db/bookdb.sql\n",
      "- embeddings/SBERT_embeddings.parquet\n",
      "- embeddings/gmf_book_embeddings.parquet\n",
      "- embeddings/gmf_user_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "list_bucket_contents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "This section focuses on loading the primary datasets required for the reranker data engineering pipeline. We will download data from Cloudflare R2 storage, load them into Dask DataFrames for efficient handling of potentially large datasets, and perform some initial exploration.\n",
    "\n",
    "The core datasets are:\n",
    "* `reduced_books.parquet`: Contains metadata for books (e.g., title, authors, genres).\n",
    "* `reduced_interactions.parquet`: Contains user-book interaction data (e.g., ratings, reads).\n",
    "* `new_authors.parquet`: Contains author information.\n",
    "* `book_texts_reduced.parquet`: Contains the textual content or descriptions of books.\n",
    "\n",
    "We will use helper functions defined earlier (`download_from_r2`, `list_bucket_contents`, `upload_to_r2`) to manage data transfer with the R2 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 'reduced_books'\n",
    "\n",
    "Reduced set of books: contains book meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/reduced_books.parquet to data/reduced_books.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/reduced_books.parquet\", \"data/reduced_books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = dd.read_parquet(\"data/reduced_books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 'reduced_interactions'\n",
    "\n",
    "Reduced set of interactions: contains data about user-book interaction data (e.g., ratings, reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/reduced_interactions.parquet to data/reduced_interactions.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/reduced_interactions.parquet\", \"data/reduced_interactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = dd.read_parquet(\"data/reduced_interactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user IDs: 205242\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique user IDs in the interactions dataframe\n",
    "unique_users_count = interactions_df['user_id'].nunique().compute()\n",
    "print(f\"Number of unique user IDs: {unique_users_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 'book_texts_reduced.parquet'\n",
    "\n",
    "Reduced set of book descriptions: contains text describing each book, used in a later script to generate training pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/book_texts_reduced.parquet to data/book_texts_reduced.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/book_texts_reduced.parquet\", \"data/book_texts_reduced.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "With the data loaded, this section focuses on transforming and engineering features to prepare for generating training pairs for the cross-encoder reranker model.\n",
    "\n",
    "The key steps include:\n",
    "* **Book Metadata Preparation (`reduced_books_df`):**\n",
    "  * Dropping irrelevant columns.\n",
    "  * Extracting and cleaning genre information from `popular_shelves`.\n",
    "  * Integrating author names using the `authors_df`.\n",
    "  * The goal is to create a concise `reduced_books_df` containing `book_id`, `title`, `description`, `genre`, and `authors`, which will be crucial for constructing user contexts and book descriptions.\n",
    "* **User Interaction Data Preparation (`interactions_df`):**\n",
    "  * Identifying unique users.\n",
    "  * Creating a `user_books_df` that lists books read by each user, sorted by rating. This helps in identifying positive examples for training.\n",
    "  * Sampling a subset of users (`sampled_users_book`) for more manageable processing, if necessary.\n",
    "\n",
    "These processed DataFrames will then be saved to Parquet format for use in the subsequent training data generation script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 'reduced_books'\n",
    "\n",
    "Preparing book metadat dataset so it can be used to generate user contexts for the cross-encoder. This is done by:\n",
    "* Dropping unnecessary columns\n",
    "* Extracting book genres\n",
    "* Adding book authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Analysis:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Non-Null Count</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Null Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>isbn</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>publisher</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>title</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>work_id</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ratings_count</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>book_id</td>\n",
       "      <td>int64</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>image_url</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>url</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>publication_year</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>edition_information</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>publication_month</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>isbn13</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>publication_day</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num_pages</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>authors</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_reviews_count</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>link</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>format</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>description</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>similar_books</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kindle_asin</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>average_rating</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is_ebook</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>asin</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>popular_shelves</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>language_code</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country_code</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>series</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>title_without_series</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Column Data Type  Non-Null Count  Null Count Null Percentage\n",
       "0                   isbn    string           17663           0           0.00%\n",
       "15             publisher    string           17663           0           0.00%\n",
       "27                 title    string           17663           0           0.00%\n",
       "26               work_id    string           17663           0           0.00%\n",
       "25         ratings_count    string           17663           0           0.00%\n",
       "24               book_id     int64           17663           0           0.00%\n",
       "23             image_url    string           17663           0           0.00%\n",
       "22                   url    string           17663           0           0.00%\n",
       "21      publication_year    string           17663           0           0.00%\n",
       "20   edition_information    string           17663           0           0.00%\n",
       "19     publication_month    string           17663           0           0.00%\n",
       "18                isbn13    string           17663           0           0.00%\n",
       "17       publication_day    string           17663           0           0.00%\n",
       "16             num_pages    string           17663           0           0.00%\n",
       "14               authors    object           17663           0           0.00%\n",
       "1     text_reviews_count    string           17663           0           0.00%\n",
       "13                  link    string           17663           0           0.00%\n",
       "12                format    string           17663           0           0.00%\n",
       "11           description    string           17663           0           0.00%\n",
       "10         similar_books    object           17663           0           0.00%\n",
       "9            kindle_asin    string           17663           0           0.00%\n",
       "8         average_rating    string           17663           0           0.00%\n",
       "7               is_ebook    string           17663           0           0.00%\n",
       "6                   asin    string           17663           0           0.00%\n",
       "5        popular_shelves    object           17663           0           0.00%\n",
       "4          language_code    string           17663           0           0.00%\n",
       "3           country_code    string           17663           0           0.00%\n",
       "2                 series    object           17663           0           0.00%\n",
       "28  title_without_series    string           17663           0           0.00%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_dataframe(df):\n",
    "    # Get column info\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    \n",
    "    # Calculate total rows\n",
    "    total_rows = len(df.compute())\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    results = []\n",
    "    \n",
    "    # Analyze each column\n",
    "    for col in cols:\n",
    "        # Count non-null values\n",
    "        non_null_count = df[col].count().compute()\n",
    "        null_count = total_rows - non_null_count\n",
    "        null_percentage = (null_count / total_rows) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'Column': col,\n",
    "            'Data Type': str(dtypes[col]),\n",
    "            'Non-Null Count': non_null_count,\n",
    "            'Null Count': null_count,\n",
    "            'Null Percentage': f'{null_percentage:.2f}%'\n",
    "        })\n",
    "    \n",
    "    # Convert results to pandas DataFrame for better display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.sort_values('Null Percentage', ascending=False)\n",
    "\n",
    "# Display the analysis\n",
    "print(\"DataFrame Analysis:\")\n",
    "display(analyze_dataframe(books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>text_reviews_count</th>\n",
       "      <th>series</th>\n",
       "      <th>country_code</th>\n",
       "      <th>language_code</th>\n",
       "      <th>popular_shelves</th>\n",
       "      <th>asin</th>\n",
       "      <th>is_ebook</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>kindle_asin</th>\n",
       "      <th>similar_books</th>\n",
       "      <th>description</th>\n",
       "      <th>format</th>\n",
       "      <th>link</th>\n",
       "      <th>authors</th>\n",
       "      <th>publisher</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>publication_day</th>\n",
       "      <th>isbn13</th>\n",
       "      <th>publication_month</th>\n",
       "      <th>edition_information</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>url</th>\n",
       "      <th>image_url</th>\n",
       "      <th>ratings_count</th>\n",
       "      <th>work_id</th>\n",
       "      <th>title</th>\n",
       "      <th>title_without_series</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>object</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>object</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>object</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>object</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: setindex, 2 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 isbn text_reviews_count  series country_code language_code popular_shelves    asin is_ebook average_rating kindle_asin similar_books description  format    link authors publisher num_pages publication_day  isbn13 publication_month edition_information publication_year     url image_url ratings_count work_id   title title_without_series\n",
       "npartitions=1                                                                                                                                                                                                                                                                                                                                                    \n",
       "               string             string  object       string        string          object  string   string         string      string        object      string  string  string  object    string    string          string  string            string              string           string  string    string        string  string  string               string\n",
       "                  ...                ...     ...          ...           ...             ...     ...      ...            ...         ...           ...         ...     ...     ...     ...       ...       ...             ...     ...               ...                 ...              ...     ...       ...           ...     ...     ...                  ...\n",
       "Dask Name: setindex, 2 expressions\n",
       "Expr=SetIndex(frame=ReadParquetFSSpec(4655b4b), _other='book_id', options={'inplace': True})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_df.set_index('book_id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines extract_genres. This function processes the popular_shelves column (which typically contains a list of dictionaries with shelf names and counts) to identify relevant genre keywords.\n",
    "\n",
    "* It uses a predefined list of genre_keywords and ignore_keywords.\n",
    "* It iterates through shelves, normalizes names, and checks for keyword presence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_genres(popular_shelves):\n",
    "    \"\"\"\n",
    "    Extracts potential genres from a list of popular shelves dictionaries,\n",
    "    adding only the base genre keyword found.\n",
    "\n",
    "    Args:\n",
    "        popular_shelves: A list of dictionaries, where each dictionary has\n",
    "                         'count' and 'name' keys.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique base genre names found, or an empty list on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(popular_shelves, np.ndarray) or len(popular_shelves) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Use a set to store unique base genres found\n",
    "        found_genres = set() \n",
    "        \n",
    "        genre_keywords = [\n",
    "            'action', 'adventure', 'comedy', 'crime', 'mystery', 'textbook', 'children', 'mathematics', 'fantasy',\n",
    "            'historical', 'horror', 'romance', 'satire', 'science fiction',\n",
    "            'scifi', 'speculative fiction', 'thriller', 'western', 'paranormal',\n",
    "            'dystopian', 'urban fantasy', 'contemporary', 'young adult', 'ya',\n",
    "            'middle grade', 'children\\'s', 'literary fiction', 'magic realism',\n",
    "            'historical fiction', 'gothic', 'suspense', 'biography', 'memoir',\n",
    "            'nonfiction', 'poetry', 'drama', 'historical romance',\n",
    "            'fantasy romance', 'romantic suspense', 'science fiction romance',\n",
    "            'contemporary romance', 'paranormal romance', 'epic fantasy',\n",
    "            'dark fantasy', 'sword and sorcery', 'steampunk', 'cyberpunk',\n",
    "            'apocalyptic', 'post-apocalyptic', 'alternate history',\n",
    "            'superhero', 'mythology', 'fairy tales', 'folklore', 'war',\n",
    "            'military fiction', 'spy fiction', 'political fiction', 'social science fiction',\n",
    "            'techno-thriller', 'medical thriller', 'legal thriller',\n",
    "            'psychological thriller', 'cozy mystery', 'hardboiled', 'noir',\n",
    "            'coming-of-age', 'lgbtq+', 'christian fiction', 'religious fiction',\n",
    "            'humor', 'travel', 'food', 'cooking', 'health', 'self-help',\n",
    "            'business', 'finance', 'history', 'science', 'technology', 'nature',\n",
    "            'art', 'music', 'philosophy', 'education', 'true crime', 'spiritual',\n",
    "            'anthology', 'short stories', 'plays', 'screenplays', 'graphic novel',\n",
    "            'comics', 'manga', 'erotica', 'new adult', 'chick lit', 'womens fiction',\n",
    "            'sports fiction', 'family saga', ' Regency romance', 'literature'\n",
    "        ]\n",
    "        # Sort keywords by length descending to match longer phrases first (e.g., \"science fiction\" before \"science\")\n",
    "        genre_keywords.sort(key=len, reverse=True)\n",
    "\n",
    "        ignore_keywords = ['to-read', 'owned', 'hardcover', 'shelfari-favorites', 'series', 'might-read',\n",
    "                           'dnf-d', 'hambly-barbara', 'strong-females', 'first-in-series',\n",
    "                           'no-thanks-series-collections-boxes', 'entertaining-but-limited',\n",
    "                           'kate-own', 'e-book', 'compliation', 'my-books',\n",
    "                           'books-i-own-but-have-not-read', 'everything-owned', 'books-to-find',\n",
    "                           'i-own-it', 'favorite', 'not-read', 'read-some-day', 'library',\n",
    "                           'audiobooks', 'status-borrowed', 'owned-books',\n",
    "                           'spec-fic-awd-locus-nom', '01', 'hardbacks', 'paper', 'german',\n",
    "                           'hardback', 'physical-scifi-fantasy', 'childhood-favorites',\n",
    "                           'bundle-same-author', 'aa-sifi-fantasy', 'ready-to-read',\n",
    "                           'bought-on-flee-markets', 'fantasy-general', 'hardcopy', 'box-2',\n",
    "                           'unfinished', 'magic', 'duplicates', 'favorites', 'books-i-own',\n",
    "                           'fantasy-classic', 'own-hard-copy', 'fantasy-read',\n",
    "                           'book-club-edition', 'sci-fi-or-fantasy', 'fiction-fantasy',\n",
    "                           'fiction-literature-poetry', 'paused-hiatus', 'status—borrowed',\n",
    "                           'recs-fantasy', 'fantasy-scifi', 'omnibus', 'speculative',\n",
    "                           'sf--fantasy', 'in-my-home-library', 'fant-myth-para-vamps',\n",
    "                           'read-in-my-20s']\n",
    "\n",
    "        for shelf in popular_shelves:\n",
    "            if not isinstance(shelf, dict) or 'name' not in shelf:\n",
    "                continue\n",
    "            \n",
    "            shelf_name = shelf['name'].lower().strip() # Normalize shelf name\n",
    "\n",
    "            # Skip if shelf name contains any ignore keywords\n",
    "            if any(ignore in shelf_name for ignore in ignore_keywords):\n",
    "                continue\n",
    "\n",
    "            # Check if any genre keyword is present in the shelf name\n",
    "            for keyword in genre_keywords:\n",
    "                # Use word boundaries or careful checks to avoid partial matches (e.g., 'art' in 'heart')\n",
    "                # Simple substring check for now, might need refinement depending on data\n",
    "                if keyword in shelf_name: \n",
    "                    found_genres.add(keyword) # Add the base keyword\n",
    "                    # Optional: break here if you only want the first/longest match per shelf\n",
    "                    # break \n",
    "\n",
    "        return sorted(list(found_genres))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_genres function: {e}\")\n",
    "        # Log the error message\n",
    "        logging.error(\"Error in extract_genres function\", exc_info=True)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/new_authors.parquet to data/new_authors.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/new_authors.parquet\", \"data/new_authors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = dd.read_parquet(\"data/new_authors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anze/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_collection.py:4392: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('popular_shelves', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n",
      "/Users/anze/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_collection.py:4392: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('authors', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of reduced books DataFrame:\n",
      "      book_id                                              title  \\\n",
      "3     6066819                               Best Friends Forever   \n",
      "15      89375  90 Minutes in Heaven: A True Story of Death an...   \n",
      "479  11731782                              Collide (Collide, #1)   \n",
      "583     54270                                         Mein Kampf   \n",
      "807     38568                         A Quick Bite (Argeneau #1)   \n",
      "\n",
      "                                           description  \\\n",
      "3    Addie Downs and Valerie Adler were eight when ...   \n",
      "15   As he is driving home from a minister's confer...   \n",
      "479  Sherry has always known there was something ou...   \n",
      "583  Madman, tyrant, animal - history has given Ado...   \n",
      "807  That hot guy tied to Lissianna Argeneau's bed?...   \n",
      "\n",
      "                                                 genre  \\\n",
      "3    coming-of-age,contemporary,drama,humor,mystery...   \n",
      "15     biography,memoir,nonfiction,self-help,spiritual   \n",
      "479  contemporary,dystopian,fantasy,paranormal,roma...   \n",
      "583  art,biography,historical,history,horror,litera...   \n",
      "807  comedy,contemporary,erotica,fantasy,humor,para...   \n",
      "\n",
      "                     authors  \n",
      "3            Jennifer Weiner  \n",
      "15   Don Piper,Cecil Murphey  \n",
      "479             Shelly Crane  \n",
      "583             Adolf Hitler  \n",
      "807             Lynsay Sands  \n",
      "\n",
      "Genre distribution:\n",
      "Dask Series Structure:\n",
      "npartitions=1\n",
      "    int64\n",
      "      ...\n",
      "Dask Name: valuecounts, 12 expressions\n",
      "Expr=(ExplodeSeries(frame=Apply(frame=(Assign(frame=Assign(frame=ReadParquetFSSpec(4655b4b)[['book_id', 'title', 'description']])))['genre'], function=<function <lambda> at 0x15e5d6e80>))).valuecounts(split_out=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anze/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/dask/dataframe/dask_expr/_collection.py:4392: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('genre', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "# Create reduced DataFrame\n",
    "reduced_books_df = books_df[['book_id', 'title', 'description']].copy()\n",
    "\n",
    "# Modify extract_genres to return a string instead of a list\n",
    "def extract_genres_string(shelves):\n",
    "    genres = extract_genres(shelves)\n",
    "    return ','.join(genres) if genres else ''\n",
    "\n",
    "# Apply the modified function to get string representation of genres\n",
    "reduced_books_df['genre'] = books_df['popular_shelves'].apply(extract_genres_string)\n",
    "\n",
    "# Convert authors to string representation as well\n",
    "def get_author_names(author_ids):\n",
    "    author_names = []\n",
    "    for author_id in author_ids:\n",
    "        try:\n",
    "            name = authors_df.loc[authors_df['author_id'] == author_id]['name'].compute().values[0]\n",
    "            author_names.append(name)\n",
    "        except:\n",
    "            continue\n",
    "    return ','.join(author_names)\n",
    "\n",
    "reduced_books_df['authors'] = books_df['authors'].apply(get_author_names)\n",
    "\n",
    "# Display sample of the reduced DataFrame\n",
    "print(\"\\nSample of reduced books DataFrame:\")\n",
    "print(reduced_books_df.head())\n",
    "\n",
    "# Display genre distribution (need to split the strings for counting)\n",
    "print(\"\\nGenre distribution:\")\n",
    "genre_counts = reduced_books_df['genre'].apply(lambda x: x.split(',') if x else []).explode().value_counts()\n",
    "print(genre_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Processed Book Metadata\n",
    "\n",
    "The `reduced_books_df` DataFrame, now containing cleaned and structured book metadata (ID, title, description, genres, authors), is saved to `data/reduce_books_df.parquet`. This file will be a key input for the `generate_training_data.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import psutil\n",
    "\n",
    "# Monitor memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Before processing\n",
    "print_memory_usage()\n",
    "\n",
    "# Set up progress bar\n",
    "with ProgressBar():\n",
    "    # Process your DataFrame\n",
    "    result = reduced_books_df.compute()  # or whatever operation you're doing\n",
    "    \n",
    "    # Save to parquet with optimizations\n",
    "    result.to_parquet(\n",
    "        'data/reduce_books_df.parquet',\n",
    "        compression='snappy',\n",
    "        index=False,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "\n",
    "# After processing\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of re-read reduced_books_df:\n",
      "    book_id                                              title  \\\n",
      "0   6066819                               Best Friends Forever   \n",
      "1     89375  90 Minutes in Heaven: A True Story of Death an...   \n",
      "2  11731782                              Collide (Collide, #1)   \n",
      "3     54270                                         Mein Kampf   \n",
      "4     38568                         A Quick Bite (Argeneau #1)   \n",
      "\n",
      "                                         description  \\\n",
      "0  Addie Downs and Valerie Adler were eight when ...   \n",
      "1  As he is driving home from a minister's confer...   \n",
      "2  Sherry has always known there was something ou...   \n",
      "3  Madman, tyrant, animal - history has given Ado...   \n",
      "4  That hot guy tied to Lissianna Argeneau's bed?...   \n",
      "\n",
      "                                               genre                  authors  \n",
      "0  coming-of-age,contemporary,drama,humor,mystery...          Jennifer Weiner  \n",
      "1    biography,memoir,nonfiction,self-help,spiritual  Don Piper,Cecil Murphey  \n",
      "2  contemporary,dystopian,fantasy,paranormal,roma...             Shelly Crane  \n",
      "3  art,biography,historical,history,horror,litera...             Adolf Hitler  \n",
      "4  comedy,contemporary,erotica,fantasy,humor,para...             Lynsay Sands  \n"
     ]
    }
   ],
   "source": [
    "test_reduced_books_read = dd.read_parquet('data/reduce_books_df.parquet')\n",
    "print(\"Sample of re-read reduced_books_df:\")\n",
    "print(test_reduced_books_read.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. reduced_interactions\n",
    "\n",
    "Now, we process the user-book interaction data. The primary goal is to create a DataFrame (`user_books_df`) where each row represents a user and contains a list of `book_id`s they have interacted with, preferably sorted by their preference (e.g., rating). This list will serve as the basis for identifying \"positive\" examples (books the user liked/read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of user_books_df:\n",
      "                            user_id  \\\n",
      "0  000b29d6db10003f526b26f03198ade6   \n",
      "1  000e8a319dcfa7d80e9a80921f8ca102   \n",
      "2  001ea9b015d2e811ef63cf70410f30f8   \n",
      "3  002263ca74da8c31029ae7ec5754083b   \n",
      "4  003b6ee4e7f465867092d92de889c8a2   \n",
      "\n",
      "                                          books_read  num_books  \n",
      "0  [22738563, 4667024, 15881, 2, 20820994, 172282...         89  \n",
      "1  [2657, 18512, 865, 7332, 31196, 4406, 68591, 1...         99  \n",
      "2  [80674, 37741, 16299991, 10964, 15881, 6, 4374...        117  \n",
      "3  [9717, 122403, 13496, 62291, 119073, 6483211, ...        188  \n",
      "4  [85733, 10407279, 74586, 13497818, 4214, 35729...        143  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_sorted_books_for_user_group(df_user_group):\n",
    "    \"\"\"\n",
    "    For a group of interactions belonging to a single user (as a pandas DataFrame),\n",
    "    sorts them by rating and returns a list of book_ids.\n",
    "    \"\"\"\n",
    "    # df_user_group is a pandas DataFrame\n",
    "    return df_user_group.sort_values('rating', ascending=False)['book_id'].tolist()\n",
    "\n",
    "\n",
    "user_books_series_dd = interactions_df.groupby('user_id').apply(\n",
    "    get_sorted_books_for_user_group,\n",
    "    meta=pd.Series(dtype='object', name='books_read_list') # Output is a Series of lists\n",
    ")\n",
    "\n",
    "user_books_ddf = user_books_series_dd.to_frame(name='books_read').reset_index()\n",
    "\n",
    "user_books_df = user_books_ddf.compute()\n",
    "\n",
    "user_books_df['num_books'] = user_books_df['books_read'].apply(len)\n",
    "\n",
    "# Display sample of the DataFrame (matches original code's intent)\n",
    "print(\"\\nSample of user_books_df:\")\n",
    "print(user_books_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(205242, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_books_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Users and Saving User-Book Lists\n",
    "\n",
    "To manage computational resources , we sample 50,000 users from the `user_books_df`. The resulting DataFrame, `sampled_users_book`, contains `user_id` and their corresponding `books_read` lists. This is saved to `data/sampled_users_book.parquet` and will also be an input to the `generate_training_data.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 50,000 users from the pandas DataFrame\n",
    "sampled_users_book_pd = user_books_df.sample(n=50000, random_state=42)\n",
    "\n",
    "# If you need it back as a Dask DataFrame\n",
    "sampled_users_book = dd.from_pandas(sampled_users_book_pd, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 42.66 MB\n",
      "[########################################] | 100% Completed | 109.71 ms\n",
      "Memory usage: 700.05 MB\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import psutil\n",
    "\n",
    "# Monitor memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Before processing\n",
    "print_memory_usage()\n",
    "\n",
    "# Set up progress bar\n",
    "with ProgressBar():\n",
    "    # Process your DataFrame\n",
    "    result = sampled_users_book.compute()  # or whatever operation you're doing\n",
    "    \n",
    "    # Save to parquet with optimizations\n",
    "    result.to_parquet(\n",
    "        'data/sampled_users_book.parquet',\n",
    "        compression='snappy',\n",
    "        index=False,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "\n",
    "# After processing\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = dd.read_parquet(\"data/sampled_users_book.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>books_read</th>\n",
       "      <th>num_books</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>[57854, 34, 7332, 5470, 9646, 14142, 11138, 17...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006260f85929db85eddee3a0bd0e504</td>\n",
       "      <td>[29056083, 357, 5358, 78129, 375802, 10428708,...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000bcda59ab565512f51f9e1f531b5e5</td>\n",
       "      <td>[862041, 2767052, 8933944, 3685, 1772910, 2641...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005f52944ea1992e95d61f287acaea9</td>\n",
       "      <td>[2219694, 169875, 18335634, 23705512, 7171637,...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000883382802f2d95a3dd545bb953882</td>\n",
       "      <td>[22402154, 13372690, 13104080, 10429045, 67523...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  \\\n",
       "0  001af7947e217e17694c5a9c097afffb   \n",
       "1  0006260f85929db85eddee3a0bd0e504   \n",
       "2  000bcda59ab565512f51f9e1f531b5e5   \n",
       "3  0005f52944ea1992e95d61f287acaea9   \n",
       "4  000883382802f2d95a3dd545bb953882   \n",
       "\n",
       "                                          books_read  num_books  \n",
       "0  [57854, 34, 7332, 5470, 9646, 14142, 11138, 17...         38  \n",
       "1  [29056083, 357, 5358, 78129, 375802, 10428708,...         20  \n",
       "2  [862041, 2767052, 8933944, 3685, 1772910, 2641...         60  \n",
       "3  [2219694, 169875, 18335634, 23705512, 7171637,...         65  \n",
       "4  [22402154, 13372690, 13104080, 10429045, 67523...        155  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Uploading Processed Data to Cloud Storage\n",
    "\n",
    "The processed DataFrames (`reduce_books_df.parquet` and `sampled_users_book.parquet`), along with the `book_texts_reduced.parquet` (which was downloaded earlier), are essential inputs for the next stage. This step uploads these files to the R2 bucket, making them accessible for the training pair generation script or other downstream processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded data/sampled_users_book.parquet to data/sampled_users_book.parquet\n",
      "Successfully uploaded data/reduce_books_df.parquet to data/reduce_books_df\n"
     ]
    }
   ],
   "source": [
    "# Upload files\n",
    "files_to_upload = [\n",
    "    (\"data/sampled_users_book.parquet\", \"data/sampled_users_book.parquet\"),\n",
    "    (\"data/reduce_books_df.parquet\", \"data/reduce_books_df\"),\n",
    "]\n",
    "upload_to_r2(files_to_upload, \"bookdbio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Verification Before Training Pair Generation\n",
    "\n",
    "Before proceeding to generate training pairs, it's crucial to verify the structure and content of the input files:\n",
    "* `data/sampled_users_book.parquet`: User IDs and their lists of read books.\n",
    "* `data/reduce_books_df.parquet`: Book metadata (ID, title, description, genre, authors).\n",
    "* `data/book_texts_reduced.parquet`: Book full texts or detailed descriptions.\n",
    "\n",
    "This step ensures the data is in the expected format for the `generate_training_data.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_r2(\"data/sampled_users_book.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User-Book Interactions ===\n",
      "Shape: (50000, 3)\n",
      "\n",
      "Columns: ['user_id', 'books_read', 'num_books']\n",
      "\n",
      "Sample:\n",
      "                            user_id  \\\n",
      "0  001af7947e217e17694c5a9c097afffb   \n",
      "1  0006260f85929db85eddee3a0bd0e504   \n",
      "2  000bcda59ab565512f51f9e1f531b5e5   \n",
      "3  0005f52944ea1992e95d61f287acaea9   \n",
      "4  000883382802f2d95a3dd545bb953882   \n",
      "\n",
      "                                          books_read  num_books  \n",
      "0  [57854, 34, 7332, 5470, 9646, 14142, 11138, 17...         38  \n",
      "1  [29056083, 357, 5358, 78129, 375802, 10428708,...         20  \n",
      "2  [862041, 2767052, 8933944, 3685, 1772910, 2641...         60  \n",
      "3  [2219694, 169875, 18335634, 23705512, 7171637,...         65  \n",
      "4  [22402154, 13372690, 13104080, 10429045, 67523...        155  \n",
      "\n",
      "Books_read column type: <class 'str'>\n",
      "Sample books_read value: [57854, 34, 7332, 5470, 9646, 14142, 11138, 17343, 30633, 92003, 23617, 46654, 15241, 18512, 77566, 538845, 1519, 665, 15997, 3836, 33, 597790, 30659, 102920, 23613, 103390, 5129, 11149, 100915, 767171, 52090, 406373, 1885, 393199, 584637, 18176747, 30851, 61535]\n",
      "\n",
      "=== Book Metadata ===\n",
      "Shape: (17663, 5)\n",
      "\n",
      "Columns: ['book_id', 'title', 'description', 'genre', 'authors']\n",
      "\n",
      "Sample:\n",
      "    book_id                                              title  \\\n",
      "0   6066819                               Best Friends Forever   \n",
      "1     89375  90 Minutes in Heaven: A True Story of Death an...   \n",
      "2  11731782                              Collide (Collide, #1)   \n",
      "3     54270                                         Mein Kampf   \n",
      "4     38568                         A Quick Bite (Argeneau #1)   \n",
      "\n",
      "                                         description  \\\n",
      "0  Addie Downs and Valerie Adler were eight when ...   \n",
      "1  As he is driving home from a minister's confer...   \n",
      "2  Sherry has always known there was something ou...   \n",
      "3  Madman, tyrant, animal - history has given Ado...   \n",
      "4  That hot guy tied to Lissianna Argeneau's bed?...   \n",
      "\n",
      "                                               genre                  authors  \n",
      "0  coming-of-age,contemporary,drama,humor,mystery...          Jennifer Weiner  \n",
      "1    biography,memoir,nonfiction,self-help,spiritual  Don Piper,Cecil Murphey  \n",
      "2  contemporary,dystopian,fantasy,paranormal,roma...             Shelly Crane  \n",
      "3  art,biography,historical,history,horror,litera...             Adolf Hitler  \n",
      "4  comedy,contemporary,erotica,fantasy,humor,para...             Lynsay Sands  \n",
      "\n",
      "=== Book Texts ===\n",
      "Shape: (17235, 2)\n",
      "\n",
      "Columns: ['book_id', 'text']\n",
      "\n",
      "Sample:\n",
      "    book_id                                               text\n",
      "0   6066819  Title: Best Friends Forever | Genres: coming-o...\n",
      "1     89375  Title: 90 Minutes in Heaven: A True Story of D...\n",
      "2  11731782  Title: Collide (Collide, #1) | Genres: contemp...\n",
      "3     54270  Title: Mein Kampf | Genres: art, biography, hi...\n",
      "4     38568  Title: A Quick Bite (Argeneau #1) | Genres: co...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load and check the input files\n",
    "print(\"=== User-Book Interactions ===\")\n",
    "user_books = dd.read_parquet('data/sampled_users_book.parquet')\n",
    "user_books_pd = user_books.compute()\n",
    "print(\"Shape:\", user_books_pd.shape)\n",
    "print(\"\\nColumns:\", user_books_pd.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(user_books_pd.head())\n",
    "print(\"\\nBooks_read column type:\", type(user_books_pd['books_read'].iloc[0]))\n",
    "print(\"Sample books_read value:\", user_books_pd['books_read'].iloc[0])\n",
    "\n",
    "print(\"\\n=== Book Metadata ===\")\n",
    "books_df = dd.read_parquet('data/reduce_books_df.parquet')\n",
    "books_pd = books_df.compute()\n",
    "print(\"Shape:\", books_pd.shape)\n",
    "print(\"\\nColumns:\", books_pd.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(books_pd.head())\n",
    "\n",
    "print(\"\\n=== Book Texts ===\")\n",
    "book_texts = dd.read_parquet('data/book_texts_reduced.parquet')\n",
    "book_texts_pd = book_texts.compute()\n",
    "print(\"Shape:\", book_texts_pd.shape)\n",
    "print(\"\\nColumns:\", book_texts_pd.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(book_texts_pd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generating Training Pairs (External Script)\n",
    "\n",
    "At this point, the necessary input files (`sampled_users_book.parquet`, `reduce_books_df.parquet`, and `book_texts_reduced.parquet`) are prepared. The next step is to generate the actual training pairs (user context, book text, label).\n",
    "\n",
    "This task is performed by the `generate_training_data.py` script.\n",
    "\n",
    "### `generate_training_data.py` Script Overview:\n",
    "\n",
    "* **Purpose:** To create positive and negative training examples for a cross-encoder reranker.\n",
    "* **Inputs:**\n",
    "    * `data/sampled_users_book.parquet`: User IDs and their lists of read books (positive interactions).\n",
    "    * `data/reduce_books_df.parquet`: Book metadata (title, authors, genres) used to create user contexts.\n",
    "    * `data/book_texts_reduced.parquet`: Textual content/descriptions for each book.\n",
    "* **Process (`cross_encoder_data_prep.py` functions):**\n",
    "    1. For each user and each book they've read (positive example):\n",
    "        * A \"user context\" is created. This context is a textual summary of the user's reading history (e.g., \"Favorite books: [Book A] by [Author X] and [Book B] by [Author Y]. Favorite genres: [Genre 1] and [Genre 2].\"). The target positive book is typically excluded from this context (leave-one-out).\n",
    "        * A positive pair is formed: `(user_context, positive_book_text, label=1)`.\n",
    "    2. For each positive example, a specified number of negative examples (`neg_ratio`) are generated:\n",
    "        * Negative books are randomly sampled from the set of all books *not* read by the user.\n",
    "        * Negative pairs are formed: `(user_context, negative_book_text, label=0)`. The user context remains the same as for the corresponding positive example.\n",
    "* **Output:**\n",
    "    * A Parquet file (e.g., `data/training_pairs.parquet/`) containing the generated training pairs, typically partitioned for easier handling. Each row includes `user_id`, `user_ctx`, `book_id`, `book_text`, and `label`.\n",
    "\n",
    "    **To run this script (from the `BookDB/reranker/` directory):**\n",
    "    ```bash\n",
    "    python generate_training_data.py\n",
    "    ```\n",
    "    This script will read the input Parquet files from the `data/` subdirectory and write its output (the `training_pairs.parquet` directory) also to `data/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post-processing Generated Training Pairs (External Script)\n",
    "\n",
    "After `generate_training_data.py` has created the initial set of training pairs (e.g., in `data/training_pairs.parquet/`), we must inspect the training paris to make sure they have been generated correctly before being used as training data for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask DataFrame loaded with 1 partitions.\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Define the base path to your Parquet directory\n",
    "base_path = 'data/training_pairs.parquet/' # Make sure this ends with a slash\n",
    "\n",
    "# List the specific parts you want to load\n",
    "parts_to_load = [\n",
    "    base_path + 'part.0.parquet',\n",
    "]\n",
    "\n",
    "# Load the specified parts\n",
    "df_dd = dd.read_parquet(parts_to_load)\n",
    "\n",
    "\n",
    "training_pairs_df = df_dd.compute()\n",
    "\n",
    "print(f\"Dask DataFrame loaded with {df_dd.npartitions} partitions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>user_ctx</th>\n",
       "      <th>book_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>57854</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Tao Te Ching | Genres: history, literat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>15808287</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Mrs. Lincoln's Dressmaker | Genres: bio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>3692</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Heart of the Matter | Genres: conte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>603515</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Hound of Rowan (The Tapestry, #1) |...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>34</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Fellowship of the Ring (The Lord of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>73965</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Drinking: A Love Story | Genres: biogra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>1215919</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Highlander Untamed (MacLeods of Skye Tr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>218038</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: All About Love (Cynster, #6) | Genres: ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>7332</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Silmarillion | Genres: adventure, a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>455930</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Echo Burning (Jack Reacher, #5) | Genre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>6145711</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Walking Dead, Vol. 10: What We Beco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>23361172</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Paperweight | Genres: contemporary, dra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>5470</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: 1984 | Genres: dystopian, fantasy, horr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>343529</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: On a Wild Night (Cynster, #8) | Genres:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>16057629</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Seducing Cinderella (Fighting for Love,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>73716</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Como agua para chocolate | Genres: cont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>9646</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Homage to Catalonia | Genres: biography...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>3077927</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: ماجدولين | Genres: drama, literature, r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>916856</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: Miss Pettigrew Lives for a Day | Genres...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>14142</td>\n",
       "      <td>Favorite books: Tao Te Ching by Lao Tzu and Gi...</td>\n",
       "      <td>Title: The Art of Loving | Genres: education, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             user_id   book_id  \\\n",
       "0   001af7947e217e17694c5a9c097afffb     57854   \n",
       "1   001af7947e217e17694c5a9c097afffb  15808287   \n",
       "2   001af7947e217e17694c5a9c097afffb      3692   \n",
       "3   001af7947e217e17694c5a9c097afffb    603515   \n",
       "4   001af7947e217e17694c5a9c097afffb        34   \n",
       "5   001af7947e217e17694c5a9c097afffb     73965   \n",
       "6   001af7947e217e17694c5a9c097afffb   1215919   \n",
       "7   001af7947e217e17694c5a9c097afffb    218038   \n",
       "8   001af7947e217e17694c5a9c097afffb      7332   \n",
       "9   001af7947e217e17694c5a9c097afffb    455930   \n",
       "10  001af7947e217e17694c5a9c097afffb   6145711   \n",
       "11  001af7947e217e17694c5a9c097afffb  23361172   \n",
       "12  001af7947e217e17694c5a9c097afffb      5470   \n",
       "13  001af7947e217e17694c5a9c097afffb    343529   \n",
       "14  001af7947e217e17694c5a9c097afffb  16057629   \n",
       "15  001af7947e217e17694c5a9c097afffb     73716   \n",
       "16  001af7947e217e17694c5a9c097afffb      9646   \n",
       "17  001af7947e217e17694c5a9c097afffb   3077927   \n",
       "18  001af7947e217e17694c5a9c097afffb    916856   \n",
       "19  001af7947e217e17694c5a9c097afffb     14142   \n",
       "\n",
       "                                             user_ctx  \\\n",
       "0   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "1   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "2   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "3   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "4   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "5   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "6   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "7   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "8   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "9   Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "10  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "11  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "12  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "13  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "14  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "15  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "16  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "17  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "18  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "19  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
       "\n",
       "                                            book_text  label  \n",
       "0   Title: Tao Te Ching | Genres: history, literat...      1  \n",
       "1   Title: Mrs. Lincoln's Dressmaker | Genres: bio...      0  \n",
       "2   Title: The Heart of the Matter | Genres: conte...      0  \n",
       "3   Title: The Hound of Rowan (The Tapestry, #1) |...      0  \n",
       "4   Title: The Fellowship of the Ring (The Lord of...      1  \n",
       "5   Title: Drinking: A Love Story | Genres: biogra...      0  \n",
       "6   Title: Highlander Untamed (MacLeods of Skye Tr...      0  \n",
       "7   Title: All About Love (Cynster, #6) | Genres: ...      0  \n",
       "8   Title: The Silmarillion | Genres: adventure, a...      1  \n",
       "9   Title: Echo Burning (Jack Reacher, #5) | Genre...      0  \n",
       "10  Title: The Walking Dead, Vol. 10: What We Beco...      0  \n",
       "11  Title: Paperweight | Genres: contemporary, dra...      0  \n",
       "12  Title: 1984 | Genres: dystopian, fantasy, horr...      1  \n",
       "13  Title: On a Wild Night (Cynster, #8) | Genres:...      0  \n",
       "14  Title: Seducing Cinderella (Fighting for Love,...      0  \n",
       "15  Title: Como agua para chocolate | Genres: cont...      0  \n",
       "16  Title: Homage to Catalonia | Genres: biography...      1  \n",
       "17  Title: ماجدولين | Genres: drama, literature, r...      0  \n",
       "18  Title: Miss Pettigrew Lives for a Day | Genres...      0  \n",
       "19  Title: The Art of Loving | Genres: education, ...      1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of loaded data: (936681, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of loaded data: {training_pairs_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (936681, 5)\n",
      "\n",
      "Columns: ['user_id', 'book_id', 'user_ctx', 'book_text', 'label']\n",
      "\n",
      "Sample of the data:\n",
      "                            user_id   book_id  \\\n",
      "0  001af7947e217e17694c5a9c097afffb     57854   \n",
      "1  001af7947e217e17694c5a9c097afffb  15808287   \n",
      "2  001af7947e217e17694c5a9c097afffb      3692   \n",
      "3  001af7947e217e17694c5a9c097afffb    603515   \n",
      "4  001af7947e217e17694c5a9c097afffb        34   \n",
      "\n",
      "                                            user_ctx  \\\n",
      "0  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "1  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "2  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "3  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "4  Favorite books: Tao Te Ching by Lao Tzu and Gi...   \n",
      "\n",
      "                                           book_text  label  \n",
      "0  Title: Tao Te Ching | Genres: history, literat...      1  \n",
      "1  Title: Mrs. Lincoln's Dressmaker | Genres: bio...      0  \n",
      "2  Title: The Heart of the Matter | Genres: conte...      0  \n",
      "3  Title: The Hound of Rowan (The Tapestry, #1) |...      0  \n",
      "4  Title: The Fellowship of the Ring (The Lord of...      1  \n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "0    698112\n",
      "1    238569\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Example user context:\n",
      "Favorite books: Tao Te Ching by Lao Tzu and Gia-Fu Feng and Jane English and Chungliang Al Huang and Rowena Pattee Kryder and Toinette Lippe, The Fellowship of the Ring (The Lord of the Rings, #1) by J.R.R. Tolkien and The Silmarillion by J.R.R. Tolkien and Christopher Tolkien and Ted Nasmith. Favorite genres: literature and history.\n",
      "\n",
      "Example book text:\n",
      "Title: Tao Te Ching | Genres: history, literature, mythology, nonfiction, philosophy, poetry, self-help, spiritual | Description: The Tao Te Ching, the esoteric but infinitely practical book written most probably in the sixth century B.C. by Lao Tsu, has been translated more frequently than any work except the Bible. This translation of the Chinese classic, which was first published twenty-five years ago, has sold more copies than any of the others. It offers the essence of each word and makes Lao Tsu's teaching immediate and alive.\n",
      "The philosophy of Lao Tsu is simple: Accept what is in front of you without wanting the situation to be other than it is. Study the natural order of things and work with it rather than against it, for to try to change what isonly sets up resistance. Nature provides everything without requiring payment or thanks, and also provides for all without discrimination--therefore let us present the same face to everyone and treat all men as equals, however they may behave. If we watch carefully, we will see that work proceeds more quickly and easily if we stop \"trying,\" if we stop putting in so much extra effort, if we stop looking for results. In the clarity of a still and open mind, truth will be reflected. We will come to appreciate the original meaning of the word \"understand,\" which means \"to stand under.\" We serve whatever or whoever stands before us, without any thought for ourselves. Te--which may be translated as \"virtue\" or \"strength\"--lies always in Tao,or\" natural law. In other words: Simply be. | Authors: Lao Tzu, Gia-Fu Feng, Jane English, Chungliang Al Huang, Rowena Pattee Kryder, Toinette Lippe\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "# Display basic information\n",
    "print(\"Shape of the dataset:\", training_pairs_df.shape)\n",
    "print(\"\\nColumns:\", training_pairs_df.columns.tolist())\n",
    "print(\"\\nSample of the data:\")\n",
    "print(training_pairs_df.head())\n",
    "\n",
    "# Check the distribution of labels\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(training_pairs_df['label'].value_counts())\n",
    "\n",
    "# Check some example user contexts and book texts\n",
    "print(\"\\nExample user context:\")\n",
    "print(training_pairs_df['user_ctx'].iloc[0])\n",
    "print(\"\\nExample book text:\")\n",
    "print(training_pairs_df['book_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of positive examples per user:\n",
      "user_id\n",
      "0005f52944ea1992e95d61f287acaea9     65\n",
      "0006260f85929db85eddee3a0bd0e504     20\n",
      "0006db397ebf02b2e891d1048fb70dbc    166\n",
      "0006de2967df1ec4432c51090803966e     76\n",
      "000883382802f2d95a3dd545bb953882    154\n",
      "                                   ... \n",
      "0a4decbebdcf1904ebea85c27c4118ef     75\n",
      "0a664903364d6bb0ae20352854e001b3     73\n",
      "0a678ecce2fdd2f3168073113dbca527    140\n",
      "0a74ac6932b75d48032dc982bb0b9841    101\n",
      "0a97f788f5707a7f116f5cc16875597e     46\n",
      "Length: 1785, dtype: int64\n",
      "\n",
      "Statistics about positive examples per user:\n",
      "count    1785.000000\n",
      "mean      133.652101\n",
      "std       100.025313\n",
      "min         1.000000\n",
      "25%        77.000000\n",
      "50%       109.000000\n",
      "75%       163.000000\n",
      "max      1478.000000\n",
      "dtype: float64\n",
      "\n",
      "Number of users with less than 5 positive examples: 14\n",
      "\n",
      "Example users with few positives:\n",
      "user_id\n",
      "008c374625966c32477ebab37e835a4e    1\n",
      "00e8157279aa30f4b919aea0a887f49a    2\n",
      "01e2d286d0361edf8c62bc580d3baa18    1\n",
      "02ac01d9ebc7165e80d8967f075adbd3    3\n",
      "0378ae8905e15ae09e01cad7c307a78b    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count positive labels per user\n",
    "positive_counts = training_pairs_df[training_pairs_df['label'] == 1].groupby('user_id').size()\n",
    "print(\"\\nNumber of positive examples per user:\")\n",
    "print(positive_counts)\n",
    "\n",
    "# Get some statistics about the positive counts\n",
    "print(\"\\nStatistics about positive examples per user:\")\n",
    "print(positive_counts.describe())\n",
    "\n",
    "# Find users with very few positive examples\n",
    "users_with_few_positives = positive_counts[positive_counts < 5]\n",
    "print(f\"\\nNumber of users with less than 5 positive examples: {len(users_with_few_positives)}\")\n",
    "print(\"\\nExample users with few positives:\")\n",
    "print(users_with_few_positives.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** What we found out from this step is that the 'generate_training_data.py' script was generating way too much data for us to feasably use in training the reranker (mistake in the way the script was written). Therefore, we reduced the training pairs dataset in a way that we ended up with around 200k-250k total pair, using multiple positive pairs from the same user as well. In order to have good data for the model to learn from, we sampled in such a way as to still have around 20k-30k unique users in the final set, meaning 20-30k unique user contexts, so the model would not overfit. For every positive pair there are 3 negative pairs, and we sampled multiple positives from each user and the corresponding number of negatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `process_parquet_chunks.py` Script Overview:\n",
    "\n",
    "*   **Purpose:** To refine the training dataset generated by `generate_training_data.py`. It processes chunks of the training data (individual Parquet files within the `data/training_pairs.parquet/` directory) to apply specific sampling logic.\n",
    "*   **Inputs:**\n",
    "        *   Individual Parquet files from a directory (e.g., `data/training_pairs.parquet/part.0.parquet`, `data/training_pairs.parquet/part.1.parquet`, etc.).\n",
    "*   **Process (`process_individual_chunk` function):**\n",
    "    1.  For each user within a chunk:\n",
    "        *   **Positive Samples:**\n",
    "            *   If a user has more positive examples than `MAX_POSITIVES_TO_SAMPLE`, it samples `MAX_POSITIVES_TO_SAMPLE` of them.\n",
    "            *   If a user has between `MIN_POSITIVES_TO_KEEP_USER` and `MAX_POSITIVES_TO_SAMPLE` (inclusive) positive examples, it keeps all of them.\n",
    "                *   If a user has fewer than `MIN_POSITIVES_TO_KEEP_USER` positive examples, their positive examples from that chunk might be dropped (depending on the logic, it seems they are dropped).\n",
    "            *   **Negative Samples:**\n",
    "                *   For each selected positive sample, it samples `NEGATIVES_PER_POSITIVE` negative samples for that same user from the available negatives *within that chunk*.\n",
    "    *   **Output:**\n",
    "        *   A single combined Parquet file (e.g., `data/processed_training_pairs_parts_0_to_12.parquet`) containing the refined and sampled training data.\n",
    "\n",
    "    **To run this script (from the `BookDB/reranker/` directory):**\n",
    "    ```bash\n",
    "    python process_parquet_chunks.py\n",
    "    ```\n",
    "    This script will read the specified `part.*.parquet` files from `data/training_pairs.parquet/`, process them, and save the combined, refined dataset to a new Parquet file in the `data/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Processed Data\n",
    "\n",
    "After running `process_parquet_chunks.py`, the final, refined dataset (e.g., `data/processed_training_pairs_parts_0_to_12.parquet`) is ready for training the cross-encoder reranker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded data/processed_training_pairs_parts_0_to_12.parquet to data/processed_training_pairs_parts_0_to_12.parquet\n"
     ]
    }
   ],
   "source": [
    "files_to_upload = [(\"data/processed_training_pairs_parts_0_to_12.parquet\", \"data/processed_training_pairs_parts_0_to_12.parquet\")]\n",
    "upload_to_r2(files_to_upload, \"bookdbio\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
