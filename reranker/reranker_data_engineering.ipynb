{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports\n",
    "\n",
    "Importing required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up functions to import data from the cloud\n",
    "* download_from_r2\n",
    "* list_bucket_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "def download_from_r2(object_name, local_path, bucket_name=\"bookdbio\"):\n",
    "    # ensure parent dir exists\n",
    "    parent_dir = os.path.dirname(local_path)\n",
    "    if parent_dir and not os.path.isdir(parent_dir):\n",
    "        os.makedirs(parent_dir, exist_ok=True)\n",
    "\n",
    "    s3 = boto3.client('s3',\n",
    "        endpoint_url = f\"https://a9a190ee80813000e18bacf626b1281b.r2.cloudflarestorage.com/\",\n",
    "        aws_access_key_id = '85fec6dd1268801ac8c1c59175ba0b76',\n",
    "        aws_secret_access_key = '798b753bab748f2c7f5e0f46fd6506b7f0b206e362b1e00055d060a72b88d55d',\n",
    "        config = Config(signature_version='s3v4')\n",
    "   )\n",
    "\n",
    "    try:\n",
    "        s3.download_file(bucket_name, object_name, local_path)\n",
    "        print(f\"Successfully downloaded {object_name} to {local_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed for {object_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket_name=\"bookdbio\"):\n",
    "    \"\"\"List all objects in the R2 bucket\"\"\"\n",
    "    s3 = boto3.client('s3',\n",
    "        endpoint_url = f\"https://a9a190ee80813000e18bacf626b1281b.r2.cloudflarestorage.com/\",\n",
    "        aws_access_key_id = '85fec6dd1268801ac8c1c59175ba0b76',\n",
    "        aws_secret_access_key = '798b753bab748f2c7f5e0f46fd6506b7f0b206e362b1e00055d060a72b88d55d',\n",
    "        config = Config(signature_version='s3v4')\n",
    "   )\n",
    "    \n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' in response:\n",
    "            print(\"Available files in bucket:\")\n",
    "            for obj in response['Contents']:\n",
    "                print(f\"- {obj['Key']}\")\n",
    "        else:\n",
    "            print(\"Bucket is empty\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing bucket contents: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_r2(files, bucket_name):\n",
    "    \"\"\"\n",
    "    Upload multiple files to Cloudflare R2 bucket\n",
    "    \n",
    "    Args:\n",
    "        files (list): List of tuples containing (file_path, object_name)\n",
    "        bucket_name (str): R2 bucket name\n",
    "    \"\"\"\n",
    "    # Configure R2 client\n",
    "    s3 = boto3.client('s3',\n",
    "        endpoint_url = f\"https://a9a190ee80813000e18bacf626b1281b.r2.cloudflarestorage.com/\",\n",
    "        aws_access_key_id = '85fec6dd1268801ac8c1c59175ba0b76',\n",
    "        aws_secret_access_key = '798b753bab748f2c7f5e0f46fd6506b7f0b206e362b1e00055d060a72b88d55d',\n",
    "        config = Config(signature_version='s3v4')\n",
    "   )\n",
    "\n",
    "    for file_path, object_name in files:\n",
    "        try:\n",
    "            s3.upload_file(file_path, bucket_name, object_name)\n",
    "            print(f\"Successfully uploaded {file_path} to {object_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Upload failed for {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bucket_contents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data From The Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. reduced_books\n",
    "\n",
    "Reduced set of books: contains book meta data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/reduced_books.parquet to data/reduced_books.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/reduced_books.parquet\", \"data/reduced_books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df = dd.read_parquet(\"data/reduced_books.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. reduced_interactions\n",
    "\n",
    "Reduced set of interactions: contains information about a users interaction with a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/reduced_interactions.parquet to data/reduced_interactions.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/reduced_interactions.parquet\", \"data/reduced_interactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df = dd.read_parquet(\"data/reduced_interactions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique user IDs: 205242\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique user IDs in the interactions dataframe\n",
    "unique_users_count = interactions_df['user_id'].nunique().compute()\n",
    "print(f\"Number of unique user IDs: {unique_users_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "Altering datasets, so they can be used to generate training inputs for the cross-encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. reduced_books\n",
    "\n",
    "Preparing book metadat dataset so it can be used to generate user contexts for the cross-encoder. This is done by:\n",
    "* Dropping unnecessary columns\n",
    "* Extracting book genres\n",
    "* Adding book authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Analysis:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Data Type</th>\n",
       "      <th>Non-Null Count</th>\n",
       "      <th>Null Count</th>\n",
       "      <th>Null Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>isbn</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>publisher</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>title</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>work_id</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ratings_count</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>book_id</td>\n",
       "      <td>int64</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>image_url</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>url</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>publication_year</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>edition_information</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>publication_month</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>isbn13</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>publication_day</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>num_pages</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>authors</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_reviews_count</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>link</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>format</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>description</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>similar_books</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kindle_asin</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>average_rating</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is_ebook</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>asin</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>popular_shelves</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>language_code</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>country_code</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>series</td>\n",
       "      <td>object</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>title_without_series</td>\n",
       "      <td>string</td>\n",
       "      <td>17663</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Column Data Type  Non-Null Count  Null Count Null Percentage\n",
       "0                   isbn    string           17663           0           0.00%\n",
       "15             publisher    string           17663           0           0.00%\n",
       "27                 title    string           17663           0           0.00%\n",
       "26               work_id    string           17663           0           0.00%\n",
       "25         ratings_count    string           17663           0           0.00%\n",
       "24               book_id     int64           17663           0           0.00%\n",
       "23             image_url    string           17663           0           0.00%\n",
       "22                   url    string           17663           0           0.00%\n",
       "21      publication_year    string           17663           0           0.00%\n",
       "20   edition_information    string           17663           0           0.00%\n",
       "19     publication_month    string           17663           0           0.00%\n",
       "18                isbn13    string           17663           0           0.00%\n",
       "17       publication_day    string           17663           0           0.00%\n",
       "16             num_pages    string           17663           0           0.00%\n",
       "14               authors    object           17663           0           0.00%\n",
       "1     text_reviews_count    string           17663           0           0.00%\n",
       "13                  link    string           17663           0           0.00%\n",
       "12                format    string           17663           0           0.00%\n",
       "11           description    string           17663           0           0.00%\n",
       "10         similar_books    object           17663           0           0.00%\n",
       "9            kindle_asin    string           17663           0           0.00%\n",
       "8         average_rating    string           17663           0           0.00%\n",
       "7               is_ebook    string           17663           0           0.00%\n",
       "6                   asin    string           17663           0           0.00%\n",
       "5        popular_shelves    object           17663           0           0.00%\n",
       "4          language_code    string           17663           0           0.00%\n",
       "3           country_code    string           17663           0           0.00%\n",
       "2                 series    object           17663           0           0.00%\n",
       "28  title_without_series    string           17663           0           0.00%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def analyze_dataframe(df):\n",
    "    # Get column info\n",
    "    cols = df.columns\n",
    "    dtypes = df.dtypes\n",
    "    \n",
    "    # Calculate total rows\n",
    "    total_rows = len(df.compute())\n",
    "    \n",
    "    # Initialize lists to store results\n",
    "    results = []\n",
    "    \n",
    "    # Analyze each column\n",
    "    for col in cols:\n",
    "        # Count non-null values\n",
    "        non_null_count = df[col].count().compute()\n",
    "        null_count = total_rows - non_null_count\n",
    "        null_percentage = (null_count / total_rows) * 100\n",
    "        \n",
    "        results.append({\n",
    "            'Column': col,\n",
    "            'Data Type': str(dtypes[col]),\n",
    "            'Non-Null Count': non_null_count,\n",
    "            'Null Count': null_count,\n",
    "            'Null Percentage': f'{null_percentage:.2f}%'\n",
    "        })\n",
    "    \n",
    "    # Convert results to pandas DataFrame for better display\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.sort_values('Null Percentage', ascending=False)\n",
    "\n",
    "# Display the analysis\n",
    "print(\"DataFrame Analysis:\")\n",
    "display(analyze_dataframe(books_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.set_index('book_id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_genres(popular_shelves):\n",
    "    \"\"\"\n",
    "    Extracts potential genres from a list of popular shelves dictionaries,\n",
    "    adding only the base genre keyword found.\n",
    "\n",
    "    Args:\n",
    "        popular_shelves: A list of dictionaries, where each dictionary has\n",
    "                         'count' and 'name' keys.\n",
    "\n",
    "    Returns:\n",
    "        A list of unique base genre names found, or an empty list on error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(popular_shelves, np.ndarray) or len(popular_shelves) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Use a set to store unique base genres found\n",
    "        found_genres = set() \n",
    "        \n",
    "        genre_keywords = [\n",
    "            'action', 'adventure', 'comedy', 'crime', 'mystery', 'textbook', 'children', 'mathematics', 'fantasy',\n",
    "            'historical', 'horror', 'romance', 'satire', 'science fiction',\n",
    "            'scifi', 'speculative fiction', 'thriller', 'western', 'paranormal',\n",
    "            'dystopian', 'urban fantasy', 'contemporary', 'young adult', 'ya',\n",
    "            'middle grade', 'children\\'s', 'literary fiction', 'magic realism',\n",
    "            'historical fiction', 'gothic', 'suspense', 'biography', 'memoir',\n",
    "            'nonfiction', 'poetry', 'drama', 'historical romance',\n",
    "            'fantasy romance', 'romantic suspense', 'science fiction romance',\n",
    "            'contemporary romance', 'paranormal romance', 'epic fantasy',\n",
    "            'dark fantasy', 'sword and sorcery', 'steampunk', 'cyberpunk',\n",
    "            'apocalyptic', 'post-apocalyptic', 'alternate history',\n",
    "            'superhero', 'mythology', 'fairy tales', 'folklore', 'war',\n",
    "            'military fiction', 'spy fiction', 'political fiction', 'social science fiction',\n",
    "            'techno-thriller', 'medical thriller', 'legal thriller',\n",
    "            'psychological thriller', 'cozy mystery', 'hardboiled', 'noir',\n",
    "            'coming-of-age', 'lgbtq+', 'christian fiction', 'religious fiction',\n",
    "            'humor', 'travel', 'food', 'cooking', 'health', 'self-help',\n",
    "            'business', 'finance', 'history', 'science', 'technology', 'nature',\n",
    "            'art', 'music', 'philosophy', 'education', 'true crime', 'spiritual',\n",
    "            'anthology', 'short stories', 'plays', 'screenplays', 'graphic novel',\n",
    "            'comics', 'manga', 'erotica', 'new adult', 'chick lit', 'womens fiction',\n",
    "            'sports fiction', 'family saga', ' Regency romance', 'literature'\n",
    "        ]\n",
    "        # Sort keywords by length descending to match longer phrases first (e.g., \"science fiction\" before \"science\")\n",
    "        genre_keywords.sort(key=len, reverse=True)\n",
    "\n",
    "        ignore_keywords = ['to-read', 'owned', 'hardcover', 'shelfari-favorites', 'series', 'might-read',\n",
    "                           'dnf-d', 'hambly-barbara', 'strong-females', 'first-in-series',\n",
    "                           'no-thanks-series-collections-boxes', 'entertaining-but-limited',\n",
    "                           'kate-own', 'e-book', 'compliation', 'my-books',\n",
    "                           'books-i-own-but-have-not-read', 'everything-owned', 'books-to-find',\n",
    "                           'i-own-it', 'favorite', 'not-read', 'read-some-day', 'library',\n",
    "                           'audiobooks', 'status-borrowed', 'owned-books',\n",
    "                           'spec-fic-awd-locus-nom', '01', 'hardbacks', 'paper', 'german',\n",
    "                           'hardback', 'physical-scifi-fantasy', 'childhood-favorites',\n",
    "                           'bundle-same-author', 'aa-sifi-fantasy', 'ready-to-read',\n",
    "                           'bought-on-flee-markets', 'fantasy-general', 'hardcopy', 'box-2',\n",
    "                           'unfinished', 'magic', 'duplicates', 'favorites', 'books-i-own',\n",
    "                           'fantasy-classic', 'own-hard-copy', 'fantasy-read',\n",
    "                           'book-club-edition', 'sci-fi-or-fantasy', 'fiction-fantasy',\n",
    "                           'fiction-literature-poetry', 'paused-hiatus', 'status—borrowed',\n",
    "                           'recs-fantasy', 'fantasy-scifi', 'omnibus', 'speculative',\n",
    "                           'sf--fantasy', 'in-my-home-library', 'fant-myth-para-vamps',\n",
    "                           'read-in-my-20s']\n",
    "\n",
    "        for shelf in popular_shelves:\n",
    "            if not isinstance(shelf, dict) or 'name' not in shelf:\n",
    "                continue\n",
    "            \n",
    "            shelf_name = shelf['name'].lower().strip() # Normalize shelf name\n",
    "\n",
    "            # Skip if shelf name contains any ignore keywords\n",
    "            if any(ignore in shelf_name for ignore in ignore_keywords):\n",
    "                continue\n",
    "\n",
    "            # Check if any genre keyword is present in the shelf name\n",
    "            for keyword in genre_keywords:\n",
    "                # Use word boundaries or careful checks to avoid partial matches (e.g., 'art' in 'heart')\n",
    "                # Simple substring check for now, might need refinement depending on data\n",
    "                if keyword in shelf_name: \n",
    "                    found_genres.add(keyword) # Add the base keyword\n",
    "                    # Optional: break here if you only want the first/longest match per shelf\n",
    "                    # break \n",
    "\n",
    "        return sorted(list(found_genres))\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extract_genres function: {e}\")\n",
    "        # Log the error message\n",
    "        logging.error(\"Error in extract_genres function\", exc_info=True)\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/new_authors.parquet to data/new_authors.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/new_authors.parquet\", \"data/new_authors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = dd.read_parquet(\"data/new_authors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced DataFrame\n",
    "reduced_books_df = books_df[['book_id', 'title', 'description']].copy()\n",
    "\n",
    "# Modify extract_genres to return a string instead of a list\n",
    "def extract_genres_string(shelves):\n",
    "    genres = extract_genres(shelves)\n",
    "    return ','.join(genres) if genres else ''\n",
    "\n",
    "# Apply the modified function to get string representation of genres\n",
    "reduced_books_df['genre'] = books_df['popular_shelves'].apply(extract_genres_string)\n",
    "\n",
    "# Convert authors to string representation as well\n",
    "def get_author_names(author_ids):\n",
    "    author_names = []\n",
    "    for author_id in author_ids:\n",
    "        try:\n",
    "            name = authors_df.loc[authors_df['author_id'] == author_id]['name'].compute().values[0]\n",
    "            author_names.append(name)\n",
    "        except:\n",
    "            continue\n",
    "    return ','.join(author_names)\n",
    "\n",
    "reduced_books_df['authors'] = books_df['authors'].apply(get_author_names)\n",
    "\n",
    "# Display sample of the reduced DataFrame\n",
    "print(\"\\nSample of reduced books DataFrame:\")\n",
    "print(reduced_books_df.head())\n",
    "\n",
    "# Display genre distribution (need to split the strings for counting)\n",
    "print(\"\\nGenre distribution:\")\n",
    "genre_counts = reduced_books_df['genre'].apply(lambda x: x.split(',') if x else []).explode().value_counts()\n",
    "print(genre_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading as parquet** to be used in the finetuning data generation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import psutil\n",
    "\n",
    "# Monitor memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Before processing\n",
    "print_memory_usage()\n",
    "\n",
    "# Set up progress bar\n",
    "with ProgressBar():\n",
    "    # Process your DataFrame\n",
    "    result = reduced_books_df.compute()  # or whatever operation you're doing\n",
    "    \n",
    "    # Save to parquet with optimizations\n",
    "    result.to_parquet(\n",
    "        'data/reduce_books_df.parquet',\n",
    "        compression='snappy',\n",
    "        index=False,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "\n",
    "# After processing\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dd.read_parquet('data/reduce_books_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with user_id and their read books sorted by rating\n",
    "def create_user_books_df(interactions_df):\n",
    "    \"\"\"\n",
    "    Create a DataFrame with user_id and their read books sorted by rating.\n",
    "    \n",
    "    Args:\n",
    "        interactions_df: DataFrame containing user-book interactions\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns 'user_id' and 'books_read'\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get all unique users\n",
    "    unique_users = interactions_df['user_id'].unique().compute().tolist()\n",
    "    \n",
    "    # Create a list to store user_id and read_books pairs\n",
    "    user_books_data = []\n",
    "    \n",
    "    # For each user, get their read books sorted by rating\n",
    "    for user_id in unique_users:\n",
    "        # Filter interactions for the specific user\n",
    "        user_interactions = interactions_df[interactions_df['user_id'] == user_id]\n",
    "        \n",
    "        # Sort by rating in descending order\n",
    "        sorted_interactions = user_interactions.sort_values(by='rating', ascending=False)\n",
    "        \n",
    "        # Get all book_ids\n",
    "        read_books = sorted_interactions['book_id'].compute().tolist()\n",
    "        \n",
    "        user_books_data.append({\n",
    "            'user_id': user_id,\n",
    "            'books_read': read_books\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame from the collected data\n",
    "    user_books_df = pd.DataFrame(user_books_data)\n",
    "    \n",
    "    return user_books_df\n",
    "\n",
    "# Create the user_books_df with all users\n",
    "user_books_df = create_user_books_df(interactions_df)\n",
    "\n",
    "# Display sample of the DataFrame\n",
    "print(\"\\nSample of user_books_df:\")\n",
    "print(user_books_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_books_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 50,000 users from the pandas DataFrame\n",
    "sampled_users_book_pd = user_books_df.sample(n=50000, random_state=42)\n",
    "\n",
    "# If you need it back as a Dask DataFrame\n",
    "sampled_users_book = dd.from_pandas(sampled_users_book_pd, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = dd.read_parquet(\"data/sampled_users_book.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>books_read</th>\n",
       "      <th>num_books</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001af7947e217e17694c5a9c097afffb</td>\n",
       "      <td>[57854, 34, 7332, 5470, 9646, 14142, 11138, 17...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0006260f85929db85eddee3a0bd0e504</td>\n",
       "      <td>[29056083, 357, 5358, 78129, 375802, 10428708,...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000bcda59ab565512f51f9e1f531b5e5</td>\n",
       "      <td>[862041, 2767052, 8933944, 3685, 1772910, 2641...</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005f52944ea1992e95d61f287acaea9</td>\n",
       "      <td>[2219694, 169875, 18335634, 23705512, 7171637,...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000883382802f2d95a3dd545bb953882</td>\n",
       "      <td>[22402154, 13372690, 13104080, 10429045, 67523...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  \\\n",
       "0  001af7947e217e17694c5a9c097afffb   \n",
       "1  0006260f85929db85eddee3a0bd0e504   \n",
       "2  000bcda59ab565512f51f9e1f531b5e5   \n",
       "3  0005f52944ea1992e95d61f287acaea9   \n",
       "4  000883382802f2d95a3dd545bb953882   \n",
       "\n",
       "                                          books_read  num_books  \n",
       "0  [57854, 34, 7332, 5470, 9646, 14142, 11138, 17...         38  \n",
       "1  [29056083, 357, 5358, 78129, 375802, 10428708,...         20  \n",
       "2  [862041, 2767052, 8933944, 3685, 1772910, 2641...         60  \n",
       "3  [2219694, 169875, 18335634, 23705512, 7171637,...         65  \n",
       "4  [22402154, 13372690, 13104080, 10429045, 67523...        155  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import psutil\n",
    "\n",
    "# Monitor memory usage\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    print(f\"Memory usage: {process.memory_info().rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Before processing\n",
    "print_memory_usage()\n",
    "\n",
    "# Set up progress bar\n",
    "with ProgressBar():\n",
    "    # Process your DataFrame\n",
    "    result = sampled_users_book.compute()  # or whatever operation you're doing\n",
    "    \n",
    "    # Save to parquet with optimizations\n",
    "    result.to_parquet(\n",
    "        'data/sampled_users_book.parquet',\n",
    "        compression='snappy',\n",
    "        index=False,\n",
    "        engine='pyarrow'\n",
    "    )\n",
    "\n",
    "# After processing\n",
    "print_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/book_texts_reduced.parquet to data/book_texts_reduced.parquet\n"
     ]
    }
   ],
   "source": [
    "download_from_r2(\"data/book_texts_reduced.parquet\", \"data/book_texts_reduced.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files\n",
    "files_to_upload = [\n",
    "    (\"data/reduce_books_df.parquet\", \"data/sampled_users_book.parquet\"),\n",
    "]\n",
    "\n",
    "upload_to_r2(files_to_upload, \"bookdbio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Data Before Running Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_r2(\"data/sampled_users_book.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User-Book Interactions ===\n",
      "Shape: (50000, 3)\n",
      "\n",
      "Columns: ['user_id', 'books_read', 'num_books']\n",
      "\n",
      "Sample:\n",
      "                            user_id  \\\n",
      "0  001af7947e217e17694c5a9c097afffb   \n",
      "1  0006260f85929db85eddee3a0bd0e504   \n",
      "2  000bcda59ab565512f51f9e1f531b5e5   \n",
      "3  0005f52944ea1992e95d61f287acaea9   \n",
      "4  000883382802f2d95a3dd545bb953882   \n",
      "\n",
      "                                          books_read  num_books  \n",
      "0  [57854, 34, 7332, 5470, 9646, 14142, 11138, 17...         38  \n",
      "1  [29056083, 357, 5358, 78129, 375802, 10428708,...         20  \n",
      "2  [862041, 2767052, 8933944, 3685, 1772910, 2641...         60  \n",
      "3  [2219694, 169875, 18335634, 23705512, 7171637,...         65  \n",
      "4  [22402154, 13372690, 13104080, 10429045, 67523...        155  \n",
      "\n",
      "Books_read column type: <class 'str'>\n",
      "Sample books_read value: [57854, 34, 7332, 5470, 9646, 14142, 11138, 17343, 30633, 92003, 23617, 46654, 15241, 18512, 77566, 538845, 1519, 665, 15997, 3836, 33, 597790, 30659, 102920, 23613, 103390, 5129, 11149, 100915, 767171, 52090, 406373, 1885, 393199, 584637, 18176747, 30851, 61535]\n",
      "\n",
      "=== Book Metadata ===\n",
      "Shape: (17663, 5)\n",
      "\n",
      "Columns: ['book_id', 'title', 'description', 'genre', 'authors']\n",
      "\n",
      "Sample:\n",
      "    book_id                                              title  \\\n",
      "0   6066819                               Best Friends Forever   \n",
      "1     89375  90 Minutes in Heaven: A True Story of Death an...   \n",
      "2  11731782                              Collide (Collide, #1)   \n",
      "3     54270                                         Mein Kampf   \n",
      "4     38568                         A Quick Bite (Argeneau #1)   \n",
      "\n",
      "                                         description  \\\n",
      "0  Addie Downs and Valerie Adler were eight when ...   \n",
      "1  As he is driving home from a minister's confer...   \n",
      "2  Sherry has always known there was something ou...   \n",
      "3  Madman, tyrant, animal - history has given Ado...   \n",
      "4  That hot guy tied to Lissianna Argeneau's bed?...   \n",
      "\n",
      "                                               genre                  authors  \n",
      "0  coming-of-age,contemporary,drama,humor,mystery...          Jennifer Weiner  \n",
      "1    biography,memoir,nonfiction,self-help,spiritual  Don Piper,Cecil Murphey  \n",
      "2  contemporary,dystopian,fantasy,paranormal,roma...             Shelly Crane  \n",
      "3  art,biography,historical,history,horror,litera...             Adolf Hitler  \n",
      "4  comedy,contemporary,erotica,fantasy,humor,para...             Lynsay Sands  \n",
      "\n",
      "=== Book Texts ===\n",
      "Shape: (17235, 2)\n",
      "\n",
      "Columns: ['book_id', 'text']\n",
      "\n",
      "Sample:\n",
      "    book_id                                               text\n",
      "0   6066819  Title: Best Friends Forever | Genres: coming-o...\n",
      "1     89375  Title: 90 Minutes in Heaven: A True Story of D...\n",
      "2  11731782  Title: Collide (Collide, #1) | Genres: contemp...\n",
      "3     54270  Title: Mein Kampf | Genres: art, biography, hi...\n",
      "4     38568  Title: A Quick Bite (Argeneau #1) | Genres: co...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load and check the input files\n",
    "print(\"=== User-Book Interactions ===\")\n",
    "user_books = dd.read_parquet('data/sampled_users_book.parquet')\n",
    "user_books_pd = user_books.compute()\n",
    "print(\"Shape:\", user_books_pd.shape)\n",
    "print(\"\\nColumns:\", user_books_pd.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(user_books_pd.head())\n",
    "print(\"\\nBooks_read column type:\", type(user_books_pd['books_read'].iloc[0]))\n",
    "print(\"Sample books_read value:\", user_books_pd['books_read'].iloc[0])\n",
    "\n",
    "print(\"\\n=== Book Metadata ===\")\n",
    "books_df = dd.read_parquet('data/reduce_books_df.parquet')\n",
    "books_pd = books_df.compute()\n",
    "print(\"Shape:\", books_pd.shape)\n",
    "print(\"\\nColumns:\", books_pd.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(books_pd.head())\n",
    "\n",
    "print(\"\\n=== Book Texts ===\")\n",
    "book_texts = dd.read_parquet('data/book_texts_reduced.parquet')\n",
    "book_texts_pd = book_texts.compute()\n",
    "print(\"Shape:\", book_texts_pd.shape)\n",
    "print(\"\\nColumns:\", book_texts_pd.columns.tolist())\n",
    "print(\"\\nSample:\")\n",
    "print(book_texts_pd.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (0, 5)\n",
      "\n",
      "Columns: ['user_id', 'book_id', 'user_ctx', 'book_text', 'label']\n",
      "\n",
      "Sample of the data:\n",
      "Empty DataFrame\n",
      "Columns: [user_id, book_id, user_ctx, book_text, label]\n",
      "Index: []\n",
      "\n",
      "Label distribution:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "Example user context:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Check some example user contexts and book texts\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExample user context:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtraining_pairs_pd\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser_ctx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExample book text:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(training_pairs_pd[\u001b[33m'\u001b[39m\u001b[33mbook_text\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1191\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1189\u001b[39m maybe_callable = com.apply_if_callable(key, \u001b[38;5;28mself\u001b[39m.obj)\n\u001b[32m   1190\u001b[39m maybe_callable = \u001b[38;5;28mself\u001b[39m._check_deprecated_callable_usage(key, maybe_callable)\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1752\u001b[39m, in \u001b[36m_iLocIndexer._getitem_axis\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot index by location index with a non-integer key\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1751\u001b[39m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._ixs(key, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/University/Y2S2/AI Machine Learning Foundations/Final Project/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Load the training pairs\n",
    "training_pairs = dd.read_parquet('data/training_pairs.parquet')\n",
    "\n",
    "# Convert to pandas for easier inspection\n",
    "training_pairs_pd = training_pairs.compute()\n",
    "\n",
    "# Display basic information\n",
    "print(\"Shape of the dataset:\", training_pairs_pd.shape)\n",
    "print(\"\\nColumns:\", training_pairs_pd.columns.tolist())\n",
    "print(\"\\nSample of the data:\")\n",
    "print(training_pairs_pd.head())\n",
    "\n",
    "# Check the distribution of labels\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(training_pairs_pd['label'].value_counts())\n",
    "\n",
    "# Check some example user contexts and book texts\n",
    "print(\"\\nExample user context:\")\n",
    "print(training_pairs_pd['user_ctx'].iloc[0])\n",
    "print(\"\\nExample book text:\")\n",
    "print(training_pairs_pd['book_text'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
