{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2353e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yamirghofran0/jupyter_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm # For progress bar\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e647aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256  # Adjust based on your available RAM and description length\n",
    "CHUNK_SIZE = 20000\n",
    "OUTPUT_PATH = \"book_texts_embeddings.parquet\" # Directory to save parquet files\n",
    "ID_COLUMN = \"book_id\" # The name of the column containing the book IDs\n",
    "TEXT_COLUMN = \"text\" # The name of the column containing the text descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed190ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Checks for MPS (Apple Silicon GPU) availability, otherwise uses CPU.\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        print(\"MPS device found. Using Apple Silicon GPU.\")\n",
    "        return torch.device(\"mps\")\n",
    "    # elif torch.cuda.is_available(): # Uncomment if you might run on NVIDIA\n",
    "    #     print(\"CUDA device found. Using NVIDIA GPU.\")\n",
    "    #     return torch.device(\"cuda\")\n",
    "    else:\n",
    "        print(\"MPS (or CUDA) not available. Using CPU.\")\n",
    "        return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f8123fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device found. Using Apple Silicon GPU.\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b7ad4c",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6fe5da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ./sbert-output/finetuning-all-MiniLM-L6-v2-books\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "model_path = '../sbert-output/finetuning-all-MiniLM-L6-v2-books'\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "try:\n",
    "    model = SentenceTransformer(model_path)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Ensure the model files are correctly placed in the 'model' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394cd3e8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9c2f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6066819</td>\n",
       "      <td>Title: Best Friends Forever | Genres: coming-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89375</td>\n",
       "      <td>Title: 90 Minutes in Heaven: A True Story of D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11731782</td>\n",
       "      <td>Title: Collide (Collide, #1) | Genres: contemp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54270</td>\n",
       "      <td>Title: Mein Kampf | Genres: art, biography, hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38568</td>\n",
       "      <td>Title: A Quick Bite (Argeneau #1) | Genres: co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    book_id                                               text\n",
       "0   6066819  Title: Best Friends Forever | Genres: coming-o...\n",
       "1     89375  Title: 90 Minutes in Heaven: A True Story of D...\n",
       "2  11731782  Title: Collide (Collide, #1) | Genres: contemp...\n",
       "3     54270  Title: Mein Kampf | Genres: art, biography, hi...\n",
       "4     38568  Title: A Quick Bite (Argeneau #1) | Genres: co..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df = dd.read_parquet(\"../data/book_texts.parquet\")\n",
    "texts_df = texts_df.compute()\n",
    "texts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a710a",
   "metadata": {},
   "source": [
    "## Generate embeddings in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d17cd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 17235 rows in chunks of 20000...\n",
      "  Processing chunk 1/1 (rows 1-17235)...\n",
      "    Created Parquet file: book_texts_embeddings.parquet with schema:\n",
      "book_id: int64\n",
      "text: large_string\n",
      "embedding: list<item: float>\n",
      "  child 0, item: float\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 417\n",
      "    Appended 17235 rows to Parquet. Total written: 17235\n",
      "\n",
      "Finished writing. Total rows processed: 17235\n"
     ]
    }
   ],
   "source": [
    "# --- Validate Input DataFrame ---\n",
    "if TEXT_COLUMN not in texts_df.columns:\n",
    "    print(f\"Error: Text column '{TEXT_COLUMN}' not found.\")\n",
    "    exit()\n",
    "columns_to_keep = [col for col in texts_df.columns if col != TEXT_COLUMN]\n",
    "if not columns_to_keep:\n",
    "    print(f\"Warning: No columns other than '{TEXT_COLUMN}' found to keep.\")\n",
    "    # Decide if this is an error or acceptable\n",
    "\n",
    "# --- Process and Append in Chunks ---\n",
    "print(f\"\\nProcessing {len(texts_df)} rows in chunks of {CHUNK_SIZE}...\")\n",
    "parquet_writer = None\n",
    "total_rows_processed = 0\n",
    "\n",
    "for i in range(0, len(texts_df), CHUNK_SIZE):\n",
    "    chunk_df = texts_df.iloc[i:min(i + CHUNK_SIZE, len(texts_df))].copy() # Get a chunk\n",
    "    print(f\"  Processing chunk {i // CHUNK_SIZE + 1}/{math.ceil(len(texts_df) / CHUNK_SIZE)} (rows {i+1}-{min(i + CHUNK_SIZE, len(texts_df))})...\")\n",
    "\n",
    "    texts_in_chunk = chunk_df[TEXT_COLUMN].tolist()\n",
    "\n",
    "    # Generate embeddings for the current chunk\n",
    "    try:\n",
    "        embeddings = model.encode(\n",
    "            texts_in_chunk,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            show_progress_bar=False, # Progress bar per chunk might be too verbose\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"    Error encoding chunk: {e}\")\n",
    "        continue # Skip this chunk or handle error differently\n",
    "\n",
    "    # Add embeddings to the chunk DataFrame\n",
    "    if len(embeddings) == len(chunk_df):\n",
    "        chunk_df['embedding'] = list(embeddings)\n",
    "    else:\n",
    "        print(f\"    Error: Embedding count mismatch for chunk. Skipping write.\")\n",
    "        continue\n",
    "\n",
    "    # Convert chunk DataFrame to Arrow Table\n",
    "    try:\n",
    "        # Explicitly define schema for embeddings if needed, especially for the first write\n",
    "        # PyArrow usually infers it well, but being explicit can prevent issues.\n",
    "        # Example schema definition (adjust dimensions):\n",
    "        # fields = [pa.field(ID_COLUMN, pa.string()), pa.field(TEXT_COLUMN, pa.string()), pa.field('embedding', pa.list_(pa.float32()))]\n",
    "        # schema = pa.schema(fields)\n",
    "        # table = pa.Table.from_pandas(chunk_df, schema=schema, preserve_index=False)\n",
    "\n",
    "        table = pa.Table.from_pandas(chunk_df, preserve_index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"    Error converting chunk to Arrow Table: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Write/Append to Parquet file\n",
    "    if parquet_writer is None:\n",
    "        # Create writer on the first chunk, inferring schema from the first table\n",
    "        try:\n",
    "            parquet_writer = pq.ParquetWriter(OUTPUT_PATH, table.schema)\n",
    "            print(f\"    Created Parquet file: {OUTPUT_PATH} with schema:\\n{table.schema}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error creating Parquet file: {e}\")\n",
    "            exit() # Stop if file creation fails\n",
    "\n",
    "    try:\n",
    "        parquet_writer.write_table(table)\n",
    "        total_rows_processed += len(chunk_df)\n",
    "        print(f\"    Appended {len(chunk_df)} rows to Parquet. Total written: {total_rows_processed}\")\n",
    "    except Exception as e:\n",
    "        print(f\"    Error writing chunk to Parquet: {e}\")\n",
    "        # Decide how to handle write errors (e.g., retry, log, stop)\n",
    "\n",
    "# --- Close Parquet Writer ---\n",
    "if parquet_writer:\n",
    "    parquet_writer.close()\n",
    "    print(f\"\\nFinished writing. Total rows processed: {total_rows_processed}\")\n",
    "else:\n",
    "    print(\"\\nNo data was written to the Parquet file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46cc2abb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6066819</td>\n",
       "      <td>Title: Best Friends Forever | Genres: coming-o...</td>\n",
       "      <td>[-0.043174773, 0.013365388, 0.03437024, 0.0240...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89375</td>\n",
       "      <td>Title: 90 Minutes in Heaven: A True Story of D...</td>\n",
       "      <td>[0.051977597, 0.08296822, -0.041392915, -0.043...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11731782</td>\n",
       "      <td>Title: Collide (Collide, #1) | Genres: contemp...</td>\n",
       "      <td>[-0.02058969, -0.10596351, 0.12436018, 0.03277...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54270</td>\n",
       "      <td>Title: Mein Kampf | Genres: art, biography, hi...</td>\n",
       "      <td>[0.038011063, 0.030201998, -0.11067172, -0.051...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38568</td>\n",
       "      <td>Title: A Quick Bite (Argeneau #1) | Genres: co...</td>\n",
       "      <td>[-0.047273964, -0.056135908, -0.0028051443, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    book_id                                               text  \\\n",
       "0   6066819  Title: Best Friends Forever | Genres: coming-o...   \n",
       "1     89375  Title: 90 Minutes in Heaven: A True Story of D...   \n",
       "2  11731782  Title: Collide (Collide, #1) | Genres: contemp...   \n",
       "3     54270  Title: Mein Kampf | Genres: art, biography, hi...   \n",
       "4     38568  Title: A Quick Bite (Argeneau #1) | Genres: co...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.043174773, 0.013365388, 0.03437024, 0.0240...  \n",
       "1  [0.051977597, 0.08296822, -0.041392915, -0.043...  \n",
       "2  [-0.02058969, -0.10596351, 0.12436018, 0.03277...  \n",
       "3  [0.038011063, 0.030201998, -0.11067172, -0.051...  \n",
       "4  [-0.047273964, -0.056135908, -0.0028051443, 0....  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_df = dd.read_parquet(OUTPUT_PATH)\n",
    "embeddings_df = embeddings_df.compute()\n",
    "embeddings_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
